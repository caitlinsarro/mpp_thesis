---
title: "Web Scraping"
subtitle: "UNSDGLEARN: Capacity Building"
author: "Caitlin Sarro"
date: "2/9/2022"
output: 
  html_document:
    toc: TRUE
    df_print: paged
    number_sections: FALSE
    highlight: tango
    theme: lumen
    toc_depth: 3
    toc_float: true
    css: custom.css 
    self_contained: false
    includes:
      after_body: footer.html
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```
***
CONTEXT TEXT

In general, remember, the basic **workflow for scraping static webpages** is the following.

**Step 1.** Load the packages

```{r, message=F}
#install.packages("")
library(RSelenium)
library(tidyverse)
library(rvest)
library(V8)


library(stringr)
library(readr)
library(shiny)

library(reshape2)
library(dplyr)

library(purrr)
library(tibble)

```



```{r}
#https://www.unssc.org/courses?f%5B0%5D=theme%3A661&f%5B1%5D=type%3A662


```

# Scraping multiple pages `r emo::ji("robot")`

Whenever you want to really understand what's going on within the functions of a new R package, it is very likely that there is a relevant article published in the [Journal of Statistical Software](https://www.jstatsoft.org/index). Let's say you are interested in how the journal was doing over the past years.

**Step 1.** Inspect the source. Basically, follow steps to extract the Xpath information.
```{r, eval=F}
#browseURL("https://www.unssc.org/courses?f%5B0%5D=theme%3A661&f%5B1%5D=type%3A662")
```

**Step 2.** Parse the page source.

```{r}

unsscpage <- "https://www.unssc.org/courses?f%5B0%5D=theme%3A661&f%5B1%5D=type%3A662"
unssc <- read_html(unsscpage)


```

```{r}
#UNFCCC 
#start RSelenium
remDr <- remoteDriver(
        remoteServerAdd = "localhost",
        port = 4445L,
        browser = "chrome"
)

remDr$open()

#navigate to your page
remDr$navigate("https://www.unssc.org/courses?f%5B0%5D=theme%3A661&f%5B1%5D=type%3A662")

#scroll down 5 times, waiting for the page to load at each time
for(i in 1:5){      
remDr$executeScript(paste("scroll(0,",i*10000,");"))
Sys.sleep(3)    
}

#get the page html
page_source <-remDr$getPageSource()

#pageEls  <- remDr$findElements(using = "css", "#contents #details #meta")

unssc <- read_html(page_source[[1]])

unssc

```
**Step 3.** Extract information.

```{r}

body_nodes <- unssc %>% 
 html_node("body") %>% 
 html_children()

body_nodes %>% 
 html_children()


parsed_nodes <- html_nodes(unssc, 
                           xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "field--type-string", " " ))]')
titles_unssc <- html_text(parsed_nodes)
titles_unssc

```


```{r}
#extract the links

weblink <- read_html(unsscpage) %>% html_nodes( 
                           xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "group-link", " " ))]') %>% html_attr("href")


weblink <- as.data.frame(weblink)


#making the website link full
weblink$weblink <- paste0("https://www.unssc.org", weblink$weblink)
weblink
```





```{r}
#Pull out longer descriptions UNSSC

weblink <- weblink$weblink

# Define the DESCRIPTION worker function
scraper <- function(weblink) {
  read_html(weblink) %>% 
    html_nodes(xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "field--type-text-long", " " ))]//*[contains(concat( " ", @class, " " ), concat( " ", "field__item", " " ))] | //*[(@id = "block-unsscwebsite-content")]//*[contains(concat( " ", @class, " " ), concat( " ", "field--label-hidden", " " ))]//p | //*[contains(concat( " ", @class, " " ), concat( " ", "page-title", " " ))]') %>% 
    html_text() %>% 
    enframe("id", "description") %>%
    mutate(weblink = weblink) 
}

# Iterate over the urls, applying the function each time
descrp_unssc <- map_dfr(weblink, scraper, .id = "id")
descrp_unssc

descrp_unssc <- descrp_unssc %>%
  group_by(id) %>%
  summarize(description = str_c(description, collapse = ", ")) %>%
    mutate(weblink = weblink) 

descrp_unssc

```

```{r}
#authors

```



```{r}
#combine 

descrp_unssc
weblink
titles_unssc <- as.data.frame(titles_unssc)
titles_unssc$id <- seq.int(nrow(titles_unssc))


weblink <- as.data.frame(weblink)
weblink$id <- seq.int(nrow(weblink))

unssc_df <- merge(titles_unssc, weblink, by="id" )
unssc_df <- merge(titles_unssc, descrp_unssc, by="id" )

unssc_df
#add in authors

unssc_df$authors <- "un"
unssc_df

```



```{r}
#export as CSV
write.csv(unssc_df,"C:/Users/caitl/Documents/GitHub/mpp_thesis/exports/unssc.csv", row.names = TRUE)

```

```{r}
#not working?
#system("sudo docker pull selenium/standalone-chrome",wait=T)
#Sys.sleep(5)
#system("sudo docker run -d -p 4445:4444 selenium/standalone-chrome",wait=T)
#Sys.sleep(5)
#remDr <- remoteDriver(port=4445L, browserName="chrome")
#Sys.sleep(15)
#remDr$open()

```