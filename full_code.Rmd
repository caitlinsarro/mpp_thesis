---
title: "Web Scraping"
subtitle: "Author Standardization"
author: "Caitlin Sarro"
date: "2/9/2022"
output: 
  html_document:
    toc: TRUE
    df_print: paged
    number_sections: FALSE
    highlight: tango
    theme: lumen
    toc_depth: 3
    toc_float: true
    css: custom.css 
    self_contained: false
    includes:
      after_body: footer.html
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```
***


**Step 1.** Load the packages `rvest` and `stringr`.

```{r, message=F}
#install.packages("")
library(RSelenium)
library(tidyverse)
library(rvest)
library(stringr)
library(readr)


library(purrr)
library(tibble)

library(data.table)

library(tidyverse)
#install.packages("broom", type="binary") 
library(tidymodels)
library(tidytext)


###do I need these?
library(tm)

library(RCurl)

library(quanteda)

#for categorization
library(quanteda)
library(tidytext)
#library(quanteda.textmodels)
#library(caret)
library(SnowballC)

library(topicmodels)


```


```{r}
#Capacity Building https://www.unsdglearn.org/courses/?_sf_s=capacity%20building
#Breaking the Silos https://www.unsdglearn.org/courses/?_sfm_sdg=5



full_code <- read_csv("exports/full_coded_long.csv")
full_code <- na.omit(full_code)

#descrp_list from courses coding
#descrp_list
#merge
#soft_coded <- merge(soft_code, descrp_list, by.x = "titles", by.y = "titles_acad", all.x = TRUE)
#soft_coded
```


**Step 3.** Extract information.

```{r}
soft_code<- read_csv("data/soft_coded_long2.csv")
weblink <- soft_code$website
# Define the DESCRIPTION worker function
scraper <- function(weblink) {
  read_html(weblink) %>% 
    html_nodes(xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "resource-contentblock", " " ))]//*[contains(concat( " ", @class, " " ), concat( " ", "wrapper", " " ))] | //*[contains(concat( " ", @class, " " ), concat( " ", "resource-contentblock", " " ))]//p') %>% 
    html_text() %>% 
    enframe("id", "description")  %>%
    mutate(weblink = weblink) 
}


# Iterate over the urls, applying the function each time
descrp <- map_dfr(weblink, scraper, .id = "id")



#Combine into single columns

descrp <- descrp %>%
 group_by(id) %>%
 summarize(description = str_c(description, collapse = ", "))


descrp$id <- as.numeric(descrp$id)

descrp[order(descrp$id, decreasing = FALSE),] 

descrp$description <- gsub("\n\t\t\t\t\tAbout this course\n\t\t\t\t\t", "", descrp$description)

descrp

```

```{r}

full_code$id <- seq.int(nrow(full_code))

#full_coded <- merge(full_code, descrp, by = "id")

#full_coded
```

```{r}

#text cleaning punctuation
full_code$description <- gsub('[[:punct:] ]+',' ',full_code$description)
#text cleaning lowercase
full_code$description <- tolower(full_code$description)

#text cleaning symbols
full_code$description <- gsub('“',' ',full_code$description)
full_code$description <- gsub('”',' ',full_code$description)
full_code$description <- gsub('‘',' ',full_code$description)
full_code$description <- gsub('’',' ',full_code$description)
full_code$description <- gsub('–',' ',full_code$description)
#remove stopwords
full_code$description = removeWords(full_code$description, stopwords("english"))
#remove whitespace
full_code$description = stripWhitespace(full_code$description)
full_code$description <- trimws(full_code$description)
full_code$description <- gsub('  ',' ',full_code$description)

head(full_code$description)
```
```{r}
#move to long format
#soft_coded_long <- soft_coded %>% 
#  mutate(category = strsplit(as.character(category), ",")) %>%
#  unnest(category)
#soft_coded_long
```

```{r}
#long export 
write.csv(full_code,"C:/Users/caitl/Documents/GitHub/mpp_thesis/exports/full_code.csv", row.names = TRUE)
```