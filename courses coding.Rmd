---
title: "Course Coding"
subtitle: ""
author: "Caitlin Sarro"
date: "2/9/2022"
output: 
  html_document:
    toc: TRUE
    df_print: paged
    number_sections: FALSE
    highlight: tango
    theme: lumen
    toc_depth: 3
    toc_float: true
    css: custom.css 
    self_contained: false
    includes:
      after_body: footer.html
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```
***


**Step 1.** Load the packages `rvest` and `stringr`.
```{r, message=F}
#install.packages("")
library(RSelenium)
library(tidyverse)
library(rvest)
library(stringr)
library(readr)

library(reshape2)
library(dplyr)

library(xml2)
library(purrr)
library(tibble)

library(data.table)

library(tm)
library(dplyr)
library(tidytext)
library(ggplot2)

library(RCurl)

library(quanteda)

#for categorization
library(quanteda)
library(tidytext)
#library(quanteda.textmodels)
#library(caret)
library(SnowballC)

library(topicmodels)
library(quanteda.dictionaries)

#plots
library(GGally)
library(ggstatsplot) #fisher's test included
library(ggmosaic) #mosaic plot

packages = c('vcd', 'vcdExtra', 'tidyverse', 'ggmosaic','RColorBrewer')

for(p in packages){
  if(!require(p, character.only = T)){
    install.packages(p)
  }
  library(p, character.only = T)
}

```



```{r}
#Capacity Building https://www.unsdglearn.org/courses/?_sf_s=capacity%20building
#Breaking the Silos https://www.unsdglearn.org/courses/?_sfm_sdg=5

coursera <- read_csv("exports/coursera_c.csv") #1 
unsdglearn <- read_csv("exports/unsdglearn.csv") #2 
unsdglearn_silos <- read_csv("exports/unsdglearn_silos.csv") #3 
unsdglearn_systems <- read_csv("exports/unsdglearn_systems.csv") #4 

sdgacademy <- read_csv("exports/sdgacademy.csv") #5

unpan <- read_csv("exports/unpan.csv") #6
unssc <- read_csv("exports/unssc.csv") #7 
unfccc <- read_csv("exports/unfccc.csv") #8
unfccc
#quick fixes
coursera$description <- coursera$keywords
#sdgacademy$titles <- sdgacademy$titles_acad
unpan$titles <- unpan$titles_unpan
unssc$titles <- unssc$titles_unssc
unfccc$titles <- unfccc$titles_unfccc

keywords_cour <- coursera %>% select(titles, description)
keywords_unsdglearn_silos <- unsdglearn_silos %>% select(titles, description) 
keywords_unsdglearn_systems <- unsdglearn_systems %>% select(titles, description) 
keywords_unsdglearn <- unsdglearn %>% select(titles, description) 
keywords_sdgacademy <- sdgacademy %>% select(titles, description)
keywords_unpan <- unpan %>% select(titles, description)
keywords_unssc <- unssc %>% select(titles, description)
keywords_unfccc <- unfccc %>% select(titles,description)
# descrp_acad


#combine into one set
combined <- rbind(keywords_cour, keywords_unsdglearn_silos, keywords_unsdglearn, keywords_sdgacademy, keywords_unpan, keywords_unssc, keywords_unsdglearn_systems)


write.csv(combined,"C:/Users/caitl/Documents/GitHub/mpp_thesis/exports/combined.csv", row.names = TRUE)

```
```{r}
#fix translations (4 translated, 3 duplicates, 2 NA's)
combined <- read_csv("data/combined_for_coding_with_translated.csv")

#import back in translated subset
#translated_courses <- read_csv("data/combined_for_coding_translated.csv")

#rename first col
#colnames(translated_courses)[1] <- "id"
#combine
#combined <- rbind(combined, translated_courses)

#combined$id <- seq.int(nrow(combined))


```

```{r}
#standardize titles 

combined$titles <- iconv(combined$titles, 'utf-8', 'ascii', sub = '')

combined$titles <- tolower(combined$titles)
combined$titles <- gsub('sustainable development goals','sdgs',combined$titles)
unique(combined$titles)

#remove duplicate trainings by titles
combined <- distinct(combined, titles, .keep_all = TRUE)

#add id value
combined$id <- seq.int(nrow(combined))

#remove row 136
combined <-as.data.frame(combined)
combined <- slice(combined, -c(20, 136))

combined

```




```{r}
#remove words that could skew analysis 


combined <- combined %>%
    mutate(description = str_remove_all(description, "participants"))



```



```{r}
#to long data
# combined <- sdgacad_df_mini

#combined <- combined %>% 
#  mutate(keywords = strsplit(as.character(keywords), ",")) %>%
#  unnest(keywords)

#trim leading and ending whitespaces
#combined$keywords <- trimws(combined$keywords)

descrp_list <- combined
#combined

#make a dataframe
#duplicate column (safety redudancy)
descrp_list$key_category <- descrp_list$description


#text cleaning punctuation
descrp_list$key_category <- gsub('[[:punct:] ]+',' ',descrp_list$key_category)
#text cleaning lowercase
descrp_list$key_category <- tolower(descrp_list$key_category)

#text cleaning symbols
descrp_list$key_category <- gsub('“',' ',descrp_list$key_category)
descrp_list$key_category <- gsub('”',' ',descrp_list$key_category)
descrp_list$key_category <- gsub('‘',' ',descrp_list$key_category)
descrp_list$key_category <- gsub('’',' ',descrp_list$key_category)
descrp_list$key_category <- gsub('–',' ',descrp_list$key_category)

#remove stopwords
descrp_list$key_category = removeWords(descrp_list$key_category, stopwords("english"))
#remove whitespace
descrp_list$key_category = stripWhitespace(descrp_list$key_category)
descrp_list$key_category <- trimws(descrp_list$key_category)
descrp_list$key_category <- gsub('  ',' ',descrp_list$key_category)

#remove unicode problem
descrp_list$key_category <- str_replace_all(descrp_list$key_category,"\\<U[^\\>]*\\>","") # only removes unicode
descrp_list$key_category <- gsub('�',' ',descrp_list$key_category)

dfmat_descrip <- dfm(tokens(descrp_list$key_category, remove_punct = TRUE))

head(descrp_list$key_category)
head(descrp_list)

```

```{r}
#test using a prefab dictionary

dict <- dictionary(file = "data/dictionary/policy_agendas_english.lcd")

#dfm(dfmat1, remove = stopwords("english"), stem = TRUE, verbose = TRUE)

dfmat_descrip <- dfm(tokens(descrp_list$key_category, remove_punct = TRUE, tolower = TRUE, stem = TRUE, verbose = TRUE))
dfm_wordstem(dfmat_descrip)
#dfmat_descrip <-dfm(dfmat_descrip, remove_punct = TRUE, tolower = TRUE, stem = TRUE, verbose = TRUE)

d_descrip <- dfm_lookup(dfmat_descrip, dict)
d_descrip <- as.data.frame(as.matrix(d_descrip))
d_descrip$id <- seq.int(nrow(d_descrip))

d_descrip


```

```{r}
#import text from  Glass, L., & Newig, J. (2019). Governance for achieving the sustainable development goals: How important are participation, policy coherence, reflexivity, adaptation and democratic institutions? Earth System Governance, 2, 100031. doi:10.1016/j.esg.2019.100031

#-------PARTICIPATION
participation <- read_csv("data/dictionary/glass newig/participation.csv")

tidy_participation<- participation %>%
    unnest_tokens("word", Terms)

#remove stopwords
data("stop_words")
    tidy_participation<-tidy_participation %>%
      anti_join(stop_words)
    
#remove punctuation
#tidy_participation<-tidy_participation[-grep("\\b\\d+\\b", tidy_participation$word),]
tidy_participation$word<- gsub("[[:punct:]]|[[:digit:]]|^http:\\/\\/.*|^https:\\/\\/.*","",tidy_participation$word)

#to lowercase
tidy_participation$word<- tolower(tidy_participation$word)

#remove whitespace
tidy_participation$word <- gsub("\\s+","",tidy_participation$word)

#stemming the keywords
tidy_participation<-tidy_participation %>%
      mutate_at("word", funs(wordStem((.), language="en")))


#see most frequent   
#tidy_participation %>%
#  count(word) %>%
#    arrange(desc(n))

#keep unique values
tidy_participation <- unique(tidy_participation)

policy_coherence <- read_csv("data/dictionary/glass newig/policy coherence.csv")
reflexivity_and_adaptation <- read_csv("data/dictionary/glass newig/reflexivity and adaptation.csv")

#terms into a character list
participation_keystems <- tidy_participation$word
participation_keystems <- as.list(tidy_participation$word)


participation_keystems <- as.list(tidy_participation)
names(participation_keystems)
participation_keystems

```








```{r}
#import text from  Glass, L., & Newig, J. (2019). Governance for achieving the sustainable development goals: How important are participation, policy coherence, reflexivity, adaptation and democratic institutions? Earth System Governance, 2, 100031. doi:10.1016/j.esg.2019.100031

#-------POLICY COHERENCE
policy_coherence <- read_csv("data/dictionary/glass newig/policy coherence.csv")

tidy_policy_c<- policy_coherence %>%
    unnest_tokens("word", Terms)

#remove stopwords
data("stop_words")
    tidy_policy_c<-tidy_policy_c %>%
      anti_join(stop_words)
    
#remove punctuation
#tidy_participation<-tidy_participation[-grep("\\b\\d+\\b", tidy_participation$word),]
tidy_policy_c$word<- gsub("[[:punct:]]|[[:digit:]]|^http:\\/\\/.*|^https:\\/\\/.*","",tidy_policy_c$word)

#to lowercase
tidy_policy_c$word<- tolower(tidy_policy_c$word)

#remove whitespace
tidy_policy_c$word <- gsub("\\s+","",tidy_policy_c$word)

#stemming the keywords
tidy_policy_c<-tidy_policy_c %>%
      mutate_at("word", funs(wordStem((.), language="en")))


#see most frequent   
#tidy_participation %>%
#  count(word) %>%
#    arrange(desc(n))

#keep unique values
tidy_policy_c <- unique(tidy_policy_c)
tidy_policy_c


#terms into a character list
policy_coh_keystems <- tidy_policy_c$word #pick one
policy_coh_keystems <- as.list(tidy_policy_c)
names(policy_coh_keystems)
policy_coh_keystems

policy_coh_keystems <- as.list(tidy_policy_c)
names(policy_coh_keystems)
policy_coh_keystems


```


```{r}
#import text from  Glass, L., & Newig, J. (2019). Governance for achieving the sustainable development goals: How important are participation, policy coherence, reflexivity, adaptation and democratic institutions? Earth System Governance, 2, 100031. doi:10.1016/j.esg.2019.100031

#-------DEMOCRATIC INSTITUTIONS
democratic_institutions <- read_csv("data/dictionary/glass newig/democratic institutions .csv")

tidy_democratic <- democratic_institutions %>%
    unnest_tokens("word", Terms)

#remove stopwords
data("stop_words")
    tidy_democratic <- tidy_democratic %>%
      anti_join(stop_words)
    
#remove punctuation
#tidy_participation<-tidy_participation[-grep("\\b\\d+\\b", tidy_participation$word),]
tidy_democratic$word <- gsub("[[:punct:]]|[[:digit:]]|^http:\\/\\/.*|^https:\\/\\/.*","", tidy_democratic$word)

#to lowercase
tidy_democratic$word<- tolower(tidy_democratic$word)

#remove whitespace
tidy_democratic$word <- gsub("\\s+","",tidy_democratic$word)

#stemming the keywords
tidy_democratic <- tidy_democratic %>%
      mutate_at("word", funs(wordStem((.), language="en")))


#see most frequent   
#tidy_participation %>%
#  count(word) %>%
#    arrange(desc(n))

#keep unique values
tidy_democratic <- unique(tidy_democratic)

policy_coherence <- read_csv("data/dictionary/glass newig/policy coherence.csv")
reflexivity_and_adaptation <- read_csv("data/dictionary/glass newig/reflexivity and adaptation.csv")




#terms into a character list
democratic_keystems <- tidy_democratic$word
democratic_keystems <- as.list(tidy_democratic$word)


democratic_keystems <- as.list(tidy_democratic)
names(democratic_keystems)
democratic_keystems
```





```{r}
#import text from  Glass, L., & Newig, J. (2019). Governance for achieving the sustainable development goals: How important are participation, policy coherence, reflexivity, adaptation and democratic institutions? Earth System Governance, 2, 100031. doi:10.1016/j.esg.2019.100031

#-------REFLEXIVITY AND ADAPTATION INSTITUTIONS
reflexivity_and_adaptation <- read_csv("data/dictionary/glass newig/reflexivity and adaptation.csv")

tidy_reflex_adapt <- reflexivity_and_adaptation %>%
    unnest_tokens("word", Terms)

#remove stopwords
data("stop_words")
    tidy_reflex_adapt <- tidy_reflex_adapt %>%
      anti_join(stop_words)
    
#remove punctuation
#tidy_participation<-tidy_participation[-grep("\\b\\d+\\b", tidy_participation$word),]
tidy_reflex_adapt$word <- gsub("[[:punct:]]|[[:digit:]]|^http:\\/\\/.*|^https:\\/\\/.*","", tidy_reflex_adapt$word)

#to lowercase
tidy_reflex_adapt$word<- tolower(tidy_reflex_adapt$word)

#remove whitespace
tidy_reflex_adapt$word <- gsub("\\s+","",tidy_reflex_adapt$word)

#stemming the keywords
tidy_reflex_adapt <- tidy_reflex_adapt %>%
      mutate_at("word", funs(wordStem((.), language="en")))


#see most frequent   
#tidy_participation %>%
#  count(word) %>%
#    arrange(desc(n))

#keep unique values
tidy_reflex_adapt <- unique(tidy_reflex_adapt)



#terms into a character list
reflex_adpt_keystems <- tidy_reflex_adapt$word
reflex_adpt_keystems <- as.list(tidy_reflex_adapt$word)


reflex_adpt_keystems <- as.list(tidy_reflex_adapt)
names(reflex_adpt_keystems)
reflex_adpt_keystems

wordStem("funding")
wordStem("templates")
wordStem("lover")
 funding templates
 
```

```{r}
#All in one dictionary
dict_descrip <- dictionary(list(participation_keystems = c("particip", "propos","relev stakehold", "communiti associ","compet","represent","stakehold", "collabor","consensus","decis", "facilit dialogu", "aspect","form", "activ", "matter","indirect", "vote", "engag", "engag stakeholder","stakeholder engag", "multi stakehold","role stakehold", "cooper", "divers inclus","participatori mechan","vulner popul", "cultur heritag", "indigen", "youth indigen","young entrepreneur"  , "reduc inequ",  "build trust",  "stakehold priorit", "civil societi",  "social inclus", "cultur divers","outreach communic", "discuss forum","convers", "interact platform","live heritag", "stakehold analysi","peer peer exchang", "interdisciplinari issu", "interdisciplinari", "delib", "delib process", "citizen","citizen engag", "repres","deliber", "panel", "institution",  "juri", "trust", "participatori","participatori process", "involv","communiti organ","communiti engag","societi", "ballot","dialogu", "communiti discuss", "inclus","youth", "assembl", "confer","facilit method","develop communic","communic design","communic import","trainer", "confid","emot",  "listen","reflect", "relationship", "rapport", "deliveri","negoti", "techniqu","deleg", "behaviour","behavior","creativ", "frontlin", "indigen", "union", "disabl","elder", "margin"
),
                             policy_coh_keystems = c("coher", "extent", "coher structur", "foster", "coordin", "coordin implement", "interministeri", "systemat", "mutual"  ,"reinforc" ,"intern cooper",  "synergi"    , "integr", "overcom",  "silo", "unintend consequ" ,    "diverg", "interrel" , "trade off" ,  "feedback loop" , "across sector", "across various sector",   "integr",   "institut chang", "strategi" ,     "polici develop",       "horizont framework" , "vertic" , "align"   ,"align polici", "balanc"  ,     "intersector interact", "intersector",  "scienc polici interfac",      "conceptu" , "coher polici plan",           "link"  , "system think",        "good practic promot",  "intersect factor",    "operationalis", "operation" ,           "prioriti"  , "approach across",  "integr coherent plan",  "assess govern institut capac",      "subnat"  ,   "aspir"  , "intergovernment partnership", "appropri polici option", "cooper framework",        "complement", "coordin offic",      "connect",  "contradictori",      "leverag point" ,      "integr plan" , "cross depart",   "strateg polici",          "comprehens system"     ,          "comprehens", "consist", "duplic" ,       "align strateg plan",       "exist resourc"   ,     "nation adapt plan"  ,      "interlink",            "lever" ,  "reiter", "contradict", "reaffirm" , "shift" , "symbiosi" ,    "transform polici pathway",   "polici pathway",     "reciproc",             "solidar"  , "polici coher" ,        "align polici decis",   "polici sequenc", "polici align" ,        "interconnect" ,        "increas coher",        "across govern depart", "institut structur" ,   "common goal" ,         "complex issu" ,        "govern depart", "transgovernance", "integr plan"),
                             
                             
                               democratic_keystems = c("elector", "media", "freedom", "civil", "liberti", "rule", "law","transpar", "percept", "credibl",  "conflict", "corrupt", "express", "fair", "legislatur", "checks bal", "respect", "independ", "judiciari", "legitim", "liber", "diplomaci", "democrat", "democraci" , "voter" , "franchis"  ,"polit economi", "pluralist" ,  "competit" , "equal" , "autocraci" , "regim", "law student", "power imbal"   ,"repres","peac", "peacebuild", "govern", "treati"  , "institut" ,"inclus process", "equal access" , "govern regul", "good govern", "polit" ,"account"  ,"effici govern"   ,"effect govern"   ,"countri"  , "reduc inequ"  ,      "collect action"  ,         "reduct"  ,          "transact" ,"public offic"  ,          "free elect"    ,           "inclus"    ,         "universal right"     ,         "franchi"  ,  "public servic",       "citizen"     , "civil liberti" ,    "democrat institut", "negoti skill" ,"ethic leadership",  "contract negoti",   "govern negoti" ,    "transact cost" ,    "human right"   ,"free elect", "polit philosoph"  , "polit polar"  ,       "repres democraci", "public account"),

                             
                                
                             
                               reflex_adpt_keystems = c("reflex", "monitor","regulatori","regulatori instrument", "interpret data", "trajectori", "perform","transdisciplinari","explor state art",  "process improv","self monitor", "strateg plan","impact assess","organiz reform","organiz adapt","use measur monitor", "evidence-bas", "self-awar", "develop strategi", "increas product" ,"long term", "reform", "assess", "share latest knowledg approach",   "identifi technic resourc", "latest develop","risk inform", "data analysi","big data","leverag big data","web data", "data collect","collect data","skill identifi act", "result base", "data applic","data design", "understand data","data inform","measur", "system chang","applic tool assess","assess valu add","structur chang","structur shift" , "ineffici deliveri","polici instrument","inform manag","python","climat model","ineffici","help compani", "scienc base", "program polici", "metric","plan cycl",  "strategic plan" ,"human resourc","guidelin", "daily practic", "long term",  "development plan", "domest","evidence bas", "progress report", "progress evalu",  "evalu", "statist","indic", "impact evalu", "evid",  "reflect","result","analysi","latest knowledg","fund templat") 
   ))
#wordStem("inefficient")


dup <-data.frame(reflex_adpt_keystems)
dup <- unique(dup)
dup$reflex_adpt_keystems


    




```

```{r}

#run dictionary against descriptions
#stemDocument(last_stem)


descrp_list <- descrp_list %>%
  mutate(stem = stemDocument(descrp_list$key_category))


dfmat_descrip <- dfm(tokens(descrp_list$stem, remove_punct = TRUE) %>% 
  tokens_ngrams(1:2))
d_descrip_ref <- dfm_lookup(dfmat_descrip, dict_descrip,  valuetype = c("glob"))
d_descrip_ref <- as.data.frame(as.matrix(d_descrip_ref))
d_descrip_ref$id <- seq.int(nrow(d_descrip_ref))

d_descrip_ref

descrp_list

```

```{r}
######


library(tokenizers)
library(stopwords)
library(tidyverse)
words_as_tokens <- setNames(lapply(sapply(descrp_list$description, 
                                 tokenize_words, 
                                 stopwords = stopwords(language = "en", source = "smart")), 
                          function(x) as.data.frame(sort(table(x), TRUE), stringsAsFactors = F)), descrp_list$titles)

# tidyverse's job
df <- words_as_tokens %>%
  bind_rows(, .id = "id") %>%
  rename(word = x)

# output

df %>% filter(Freq >= 10)
```

```{r}
#quick check on which words are the matched per category 


toks <- tokens(tail(descrp_list$stem, remove_punct = TRUE)) %>% 
  tokens_ngrams(1:2)

dfm_list <- list()
for (key in names(dict_descrip)) {
  this_dfm <- tokens_select(toks, dict_descrip[key], pad = TRUE) %>%
    tokens_compound(dict_descrip[key]) %>%
    tokens_replace("", "OTHER") %>%
    dfm(tolower = FALSE)
  dfm_list <- c(dfm_list, this_dfm)
}
names(dfm_list) <- names(dict_descrip)
dfm_list

dfm_list_df <- as.data.frame(dfm_list)
```

```{r}

#d_descrip_demo
#d_descrip_part
#d_descrip_pol
#d_descrip_ref


#descrip_comb <- left_join(d_descrip_demo,d_descrip_part, by = "id") 
#descrip_comb <- left_join(descrip_comb,d_descrip_pol, by = "id") 
#descrip_comb <- left_join(descrip_comb,d_descrip_ref, by = "id")

#descrip_comb

#adding index as a col for matching
descrp_list$id <- seq.int(nrow(descrp_list))

#descrp_list <- sdgacad_df_mini
descrip_comb <- left_join(descrp_list,d_descrip_ref, by = "id") 

```

```{r}
#assigning a category label per course

descrip_comb$max.col <- descrip_comb %>% select(participation_keystems:reflex_adpt_keystems) %>% {names(.)[max.col(.)]}
descrip_comb

#how many of each
table(descrip_comb$max.col)

```
```{r}
#wider export (useful for checking)
write.csv(descrip_comb,"C:/Users/caitl/Documents/GitHub/mpp_thesis/exports/descrip_comb.csv", row.names = TRUE)
descrip_comb


```
```{r}
#combine with soft code
soft_coded_long <- read_csv("data/soft_coded_long2.csv")

#lowercase titles
#text cleaning lowercase
soft_coded_long$titles <- tolower(soft_coded_long$titles)

#fix one entry that keeps breaking the code

soft_coded_long$category[soft_coded_long$titles == "m.a. development studies and diplomacy"] <- "democratic institutions"

```
```{r}
hard_coded <- read_csv("exports/descrip_comb.csv")

#add in new columns
hard_coded$category <- "category"
hard_coded$major_categ <- 4

#transform info to columns
hard_coded$category[hard_coded$max.col == "participation_keystems"] <- "participation"
hard_coded$category[hard_coded$max.col == "policy_coh_keystems"] <- "policy coherence"
hard_coded$category[hard_coded$max.col == "democratic_keystems"] <- "democratic institutions"
hard_coded$category[hard_coded$max.col == "reflex_adpt_keystems"] <- "reflexivity and adaptation"

hard_coded$major_categ[hard_coded$max.col == "participation_keystems"] <- 1
hard_coded$major_categ[hard_coded$max.col == "policy_coh_keystems"] <- 2
hard_coded$major_categ[hard_coded$max.col == "democratic_keystems"] <- 3
hard_coded$major_categ[hard_coded$max.col == "reflex_adpt_keystems"] <- 4
#convert to numeric
as.numeric(hard_coded$major_categ)

hard_coded$id <- seq.int(nrow(hard_coded))
hard_coded <- hard_coded %>% select(id, titles, category,description, major_categ)
soft_coded_long <- soft_coded_long %>% select(id, titles, category,description, major_categ)
#combine them
all_courses_coded <- rbind(soft_coded_long, hard_coded)
all_courses_coded$id <- seq.int(nrow(all_courses_coded))
#uniques
all_courses_coded <- unique(all_courses_coded)
#convert to numeric
as.factor(all_courses_coded$major_categ)


#text cleaning punctuation
all_courses_coded$description <- gsub('[[:punct:] ]+',' ',all_courses_coded$description)
#text cleaning lowercase
all_courses_coded$description <- tolower(all_courses_coded$description)
#text cleaning symbols
all_courses_coded$description <- gsub(',',' ',all_courses_coded$description)

#remove stopwords
all_courses_coded$description = removeWords(all_courses_coded$description, stopwords("english"))


#export to fix problems
write.csv(all_courses_coded,"C:/Users/caitl/Documents/GitHub/mpp_thesis/exports/all_courses_coded_export.csv", row.names = TRUE)
```

```{r}
all_courses_coded
final_coded_dataset <- read_csv("data/final_coded_dataset.csv")
#are the dupes from softcoding?
recoded <- select(final_coded_dataset, -c("category", "major_categ"))
all_courses_coded <- select(all_courses_coded, -c("description", "id"))

recoded <- left_join(recoded, all_courses_coded, by = "titles", all.x=TRUE)

recoded <- unique(recoded)

recoded
#export to fix problems
write.csv(recoded,"C:/Users/caitl/Documents/GitHub/mpp_thesis/exports/recoded.csv", row.names = TRUE)

```



```{r}
#I AM HEREEE
#check top keywords per topic
#Relative frequency analysis (keyness)
dfmat_descrip
names(dfm_list) <- names(dict_descrip)
dfm_list

dfm_list_df

dfmat_descrip_trim <- dfm_trim(dfmat_descrip, min_termfreq = 10, min_docfreq = 5)

dfmat_descrip_trim

docvars(dfmat_descrip)

dfmat_descrip
dict_descrip
dfm_list


class(dfmat_descrip)

######

descrip_comb
top_words_partic <- filter(descrip_comb, max.col == "participation_keystems") 
top_words_pol_co <- filter(descrip_comb, max.col == "policy_coh_keystems") 
top_words_demo <- filter(descrip_comb, max.col == "democratic_keystems") 
top_words_reflex <- filter(descrip_comb, max.col == "reflex_adpt_keystems") 


descrip_comb

words_as_tokens <-  setNames(lapply(sapply(top_words_partic$description, 
                                 tokenize_words, 
                                 stopwords = stopwords(language = "en", source = "smart")), 
                          function(x) as.data.frame(sort(table(x), TRUE), stringsAsFactors = F)), top_words_partic$titles)

# tidyverse's job
df <- words_as_tokens %>%
  bind_rows(, .id = "id") %>%
  rename(word = x)

# output

df %>% filter(Freq >= 8) %>% arrange(desc(Freq))




```


```{r}
#check keywords in context



```




###start to combine with authors

```{r}
#try to combine with authors
#descrip_comb
#au_category_clean

au_category_clean <- read_csv("exports/au_category_clean.csv")

#clean up a bit
au_category_clean <- au_category_clean %>%
 group_by(titles, au_category) %>%
 summarize(authors = str_c(authors, collapse = ", ")) 



#standardize the titles across the two sheets
au_category_clean$titles <- iconv(au_category_clean$titles, 'utf-8', 'ascii', sub = '')

au_category_clean$titles <- tolower(au_category_clean$titles)
au_category_clean$titles <- gsub('sustainable development goals','sdgs',au_category_clean$titles)
#remove white spaces
au_category_clean$titles <- trimws(au_category_clean$titles)
all_courses_coded$titles <- trimws(all_courses_coded$titles)


#collapse au_category
au_category_clean <- au_category_clean %>%
 group_by(titles) %>%
 summarize(au_category = str_c(au_category, collapse = ", ")) 
```

```{r}
#join them together
au_category_clean
all_courses_coded

au_diff_coding <- left_join(all_courses_coded, au_category_clean, by= "titles", all.x =TRUE)
#au_diff_coding <- au_diff_coding %>% select (titles, category, description, au_category)


#export out to investigate final problems (duplicates, coding error, nas)
write.csv(au_diff_coding,"C:/Users/caitl/Documents/GitHub/mpp_thesis/exports/au_diff_coding.csv", row.names = TRUE)
final_coded_dataset <- read_csv("data/final_coded_dataset_.csv")
```


```{r}

#a lot of missing au_category for some reason --- reattach author list

coursera <- read_csv("exports/coursera_c.csv")
unsdglearn_silos <- read_csv("exports/unsdglearn_silos.csv")
unsdglearn_systems <- read_csv("exports/unsdglearn_systems.csv")
unsdglearn <- read_csv("exports/unsdglearn.csv")
sdgacademy <- read_csv("exports/sdgacademy.csv")
unpan <- read_csv("exports/unpan.csv")
unssc <- read_csv("exports/unssc.csv")
unfccc <- read_csv("exports/unfccc.csv")

authors_cour <- coursera %>% select(titles, author)
authors_unsdglearn_silos <- unsdglearn_silos %>% select(titles, authors) #fix this s
authors_unsdglearn_systems <- unsdglearn_systems %>% select(titles, authors) #fix this s
authors_unsdglearn <- unsdglearn %>% select(titles, authors) #fix this s
authors_sdgacademy <- sdgacademy %>% select(titles, author) #fix this standardize
authors_unpan <- unpan %>% select(titles_unpan, authors_unpan)
authors_unssc <- unssc %>% select(titles_unssc, authors)
authors_unfccc <- unfccc %>% select(titles_unfccc, author)

#fix nonstandard names to titles and authors
authors_unpan <- authors_unpan %>%
  rename(titles = titles_unpan)

authors_unssc <- authors_unssc %>%
  rename(titles = titles_unssc)

authors_unfccc <- authors_unfccc %>%
  rename(titles = titles_unfccc)

## authors

authors_sdgacademy <- authors_sdgacademy %>%
  rename(authors = author)

authors_cour <- authors_cour %>%
  rename(authors = author)

authors_unpan <- authors_unpan %>%
  rename(authors = authors_unpan)

authors_unfccc <- authors_unfccc %>%
  rename(authors = author)

#combine into one set
combined <- rbind(authors_cour, authors_unsdglearn_silos, authors_unsdglearn, authors_sdgacademy, authors_unpan, authors_unsdglearn_systems, authors_unssc, authors_unfccc)

#standardize the titles across the two sheets
combined$titles <- iconv(combined$titles, 'utf-8', 'ascii', sub = '')

combined$titles <- tolower(combined$titles)
combined$titles <- gsub('sustainable development goals','sdgs',combined$titles)
#remove white spaces
combined$titles <- trimws(combined$titles)
combined$titles <- trimws(combined$titles)


final_coded_dataset <- left_join(final_coded_dataset, combined, by= "titles", all.x =TRUE)

final_coded_dataset


#export out to fix au_category problem by author
write.csv(final_coded_dataset,"C:/Users/caitl/Documents/GitHub/mpp_thesis/exports/final_coded_dataset_.csv", row.names = TRUE)
final_coded_dataset <- read_csv("data/final_coded_dataset.csv")
# 212 
# Final coded dataset
```


```{r}

#how many of each
table(final_coded_dataset$category)

```

```{r}
#code out UN ONLY
au_un_only <- read_csv("data/au_diff_coding_un_only.csv")
au_un_only <- merge(descrip_comb, au_un_only, by= "titles", all.y = TRUE)
au_un_only <- au_un_only %>% select (titles, max.col)

#code out NON UN (only)
au_main <- read_csv("data/au_diff_coding_removed_un_only.csv")
au_main <- merge(descrip_comb, au_main, by= "titles", all.y= TRUE)
au_main <- au_main %>% select (titles, max.col)
unfccc

au_diff_coding_export <- au_diff_coding %>%
 group_by(titles) %>%
 summarize(au_category = str_c(au_category, collapse = ", "))

au_diff_coding_export

write.csv(au_un_only,"C:/Users/caitl/Documents/GitHub/mpp_thesis/exports/au_un_only.csv", row.names = TRUE)
write.csv(au_main,"C:/Users/caitl/Documents/GitHub/mpp_thesis/exports/au_main.csv", row.names = TRUE)
```


```{r}

all_courses_coded <- read_csv("data/final_coded_dataset.csv")
table(all_courses_coded$category)
#to long data
all_courses_coded_long <- all_courses_coded %>% 
  mutate(au_category = strsplit(as.character(au_category), ",")) %>%
  unnest(au_category)


#remove white spaces
all_courses_coded_long$au_category <- trimws(all_courses_coded_long$au_category)


all_courses_coded_long<- as.data.frame(all_courses_coded_long, headers=TRUE)

write.csv(all_courses_coded_long,"C:/Users/caitl/Documents/GitHub/mpp_thesis/exports/all_courses_coded_long.csv", row.names = TRUE)


#simply categories
all_courses_coded_long$au_category[all_courses_coded_long$au_category == "cultural institution"] <- "civil society"
all_courses_coded_long$au_category[all_courses_coded_long$au_category == "ngo"] <- "civil society"
all_courses_coded_long$au_category[all_courses_coded_long$au_category == "business"] <- "research  business"
all_courses_coded_long$au_category[all_courses_coded_long$au_category == "finance"] <- "research  business"
all_courses_coded_long$au_category[all_courses_coded_long$au_category == "research  policy institutes"] <- "research  business"
all_courses_coded_long$au_category[all_courses_coded_long$au_category == "un ngo"] <- "civil society"

# add in un only courses

#au_diff_allcourses <- rbind(au_diff_main_long, au_diff_un_only)

#table 3. Contingency table
freq_table <- table(all_courses_coded_long$au_category, all_courses_coded_long$category)

chisq.test(freq_table)
chisq.test(all_courses_coded_long$au_category, all_courses_coded_long$category, correct=FALSE)

#X-squared = 35.32, df = 15, p-value = 0.002214
authors <- all_courses_coded_long$au_category
govern <- all_courses_coded_long$category

#freq_table <- au_diff_allcourses %>% select(id.x, au_category, max.col)

#freq_table <- na.omit(freq_table)



author_plots <- all_courses_coded_long %>%
 group_by(titles, category) %>%
 summarize(au_category = str_c(au_category, collapse = ", "))

author_plots
write.csv(author_plots,"C:/Users/caitl/Documents/GitHub/mpp_thesis/exports/author_plots.csv", row.names = TRUE)
```

```{r}
chisq.test(freq_table)$expected

```
```{r}
write.csv(freq_table,"C:/Users/caitl/Documents/GitHub/mpp_thesis/exports/final_freq_table.csv", row.names = TRUE)
```



```{r}
# Fisher's exact test
fishtest <- fisher.test(freq_table,simulate.p.value=TRUE,B=1e7)
fishtest

```






```{r}
#mosaic plot
all_courses_coded
#to long data
all_courses_coded_long <- all_courses_coded %>%
  mutate(category = category) %>%
  mutate(au_category = strsplit(as.character(au_category), ",")) %>%
  unnest(au_category)


mosaic(~ au_category + category, data = all_courses_coded_long,
main = "Percentage Distribution of Authors \n contributing to Sustainable Governance Trainings", shade = TRUE, legend = TRUE)

freq_table



mosaicplot(freq_table,
  main = "\n Percentage Distribution of Authors \n contributing to Sustainable Governance Trainings\n", 
  sub = paste0(
    "Fisher's exact test p-value = < 0.001"),
  color = TRUE
)

```

```{r}
all_courses_coded <- read_csv("data/final_coded_dataset.csv")
ggplot(data = all_courses_coded) +
  geom_mosaic(aes(x = product(au_category, category), fill=au_category)) +
  theme_mosaic()

ggplot(data = all_courses_coded) +
  geom_mosaic(aes(x=product(au_category, category), fill = category)) +
  theme_mosaic()

all_courses_coded

```

```{r}
# The palette with purple:
cbp1 <- c("#88CCEE", "#44AA99", "#117733", "#AA4499",
          "#CC6677", "#661100")


spineplot(freq_table, col = cbp1,
          border = "#FFFFFF", main = "\n Percentage Distribution of Authors \n contributing to Sustainable Governance Trainings\n", xlab= "test", ylab="test") +
  theme(axis.text.y = element_blank(),
        axis.ticks.y=element_blank(),
        axis.text.x = element_text(angle = 90),
        legend.position = "right")

```


```{r}
all_courses_coded
# combine plot and statistical test with ggbarstats
au_diff_allcourses_simple <- all_courses_coded %>% select(id, au_category, category)

au_diff_allcourses_simple <- na.omit(au_diff_allcourses_simple)



ggbarstats(
  au_diff_allcourses_simple,au_category,category,
  results.subtitle = FALSE,
  subtitle = paste0(
    "Fisher's exact test", ", p-value = ",
    ifelse(fishtest$p.value < 0.001, "< 0.001", round(fishtest$p.value, 3))
  )
)


```





```{r}

#summary table
au_matrix <- table(au_diff_main_long$max.col, au_diff_main_long$au_category)
#table to dataframe
au_matrix <- as.data.frame(au_matrix)
au_matrix


#change to characters
au_matrix$Var1 <- as.character(au_matrix$Var1)
au_matrix$Var2 <- as.character(au_matrix$Var2)

#simply categories
au_matrix$Var2[au_matrix$Var2 == "cultural institution"] <- "civil society"
au_matrix$Var2[au_matrix$Var2 == "ngo"] <- "civil society"
au_matrix$Var2[au_matrix$Var2 == "business"] <- "research  business"
au_matrix$Var2[au_matrix$Var2 == "finance"] <- "research  business"
au_matrix$facet[au_matrix$Var2 == "research policy institutes"] <- "research  business"
au_matrix$Var2[au_matrix$Var2 == "un ngo"] <- "civil society"


#create facet groups
au_matrix$facet[au_matrix$Var2 == "private university"] <- "Gr1"
au_matrix$facet[au_matrix$Var2 == "public university"] <- "Gr1"
au_matrix$facet[au_matrix$Var2 == "research  business"] <- "Gr2"
au_matrix$facet[au_matrix$Var2 == "civil society"] <- "Gr3"
au_matrix$facet[au_matrix$Var2 == "government"] <- "Gr3"
au_matrix$facet[au_matrix$Var2 == "un"] <- "Gr4"

#sum up repeats
au_matrix <- au_matrix %>% 
  group_by(Var1, Var2, facet) %>%
  summarize_all(sum)

au_matrix

write.csv(au_matrix,"C:/Users/caitl/Documents/GitHub/mpp_thesis/exports/au_matrix.csv", row.names = TRUE)
au_matrix_simple <- read_csv("data/au_matrix_simplified.csv")
au_matrix_simple
#fix/rename cols
#au_matrix <- colnames(au_matrix$Var1)[1] <- "category"
#au_matrix <- colnames(au_matrix$Var2)[2] <- "author"

au_matrix_simple




```

```{r}
au_diff_main_long


Estimate e|ect of covariates
# estimate effect
stm_effects <- estimateEffect(au_category ~ max.col,
stmobj = stm_object,
meta = stm_blogs$meta,
uncertainty = "Global")
summary(stm_effects)



```

```{r}
library(ggplot2)
 
# create a dataset
specie <- c(rep("sorgho" , 3) , rep("poacee" , 3) , rep("banana" , 3) , rep("triticum" , 3) )
condition <- rep(c("normal" , "stress" , "Nitrogen") , 4)
value <- abs(rnorm(12 , 0 , 15))
data <- data.frame(specie,condition,value)
 
# Stacked
ggplot(au_matrix_simple, aes(fill=Var1, y=Freq, x=Var2)) + 
    geom_bar(position="stack", stat="identity")

au_matrix_simple
au_matrix



```

```{r}

ggplot(au_matrix, aes(fill=Var1, y=Freq, x=Var2)) + 
    geom_bar(position="stack", stat="identity")

# Quick display of two cabapilities of GGally, to assess the distribution and correlation of variables 

 
# Create data 
data <- data.frame( var1 = 1:100 + rnorm(100,sd=20), v2 = 1:100 + rnorm(100,sd=27), v3 = rep(1, 100) + rnorm(100, sd = 1)) 
data$v4 = data$var1 ** 2 
data$v5 = -(data$var1 ** 2) 
 
# Check correlations (as scatterplots), distribution and print corrleation coefficient 
ggpairs(au_matrix, title="correlogram with ggpairs()") 

ggcorr(au_matrix, method="circle")

au_matrix




line_unonly = lm(Var1 ~ Freq )'data= au_matrix_un, aes(x=Var1, y=Freq, group = Var1), color = "black"'

au_matrix
#do the plot
ggplot(au_matrix, aes(fill=Var2, y=Freq, x=Var1)) +
  geom_point(size=2, shape=23) +
  geom_line(aes(group = Var2), color="gray") +
  theme_minimal()  + facet_wrap( ~ facet) +
  geom_line(data = au_matrix_un, aes(x=Var1, y=Freq))
au_matrix


line2 = data.frame(x=xl,y=yl,type)

ggplot(data, aes(x,y)) + geom_line() + facet_wrap(~type) +
   geom_line(data = line_unonly)

au_matrix_un

###
au_matrix
#do the plot
ggplot(au_matrix, aes(fill=Var2, y=Freq, x=Var1)) +
  geom_point(size=2, shape=23) +
  geom_line(aes(group = Var2), color="gray") +
  theme_minimal()  + facet_wrap( ~ facet)  +
  geom_point(data = au_matrix_un, aes(x=Var1, y=Freq)) +
  geom_point(size=2, shape=23)
au_matrix


xl    = c( 4,  1)
yl    = c( 1,  4)
type =rep(LETTERS[1:4], each=2)
line2 = data.frame(x=au_matrix_un$Var1,y=au_matrix_un$Freq,au_matrix_un)

ggplot(data, aes(x,y)) + geom_line() + facet_wrap(~type) +
   geom_line(data = line2)
au_matrix_un

```


```{r}
#old code



descrip_comb$result = descrip_comb %>% select(participation_keystems, policy_coh_keystems, democratic_keystems, reflex_adpt_keystems) %>% names(descrip_comb)[apply(descrip_comb, 1, which.max)]

descrip_comb$result <- colnames(descrip_comb)[apply(max.col = colnames(pmax(participation_keystems, policy_coh_keystems, democratic_keystems, reflex_adpt_keystems), MARGIN = 4))]


descrip_comb_test <- descrip_comb %>% select( -id) %>% colnames(descrip_comb)[apply(descrip_comb,1,which.max)]

colnames(descrip_comb_mini)[apply(descrip_comb_mini,1,which.max)]

descrip_comb %>% 
  mutate(max.col = colnames(pmax(participation_keystems, policy_coh_keystems, democratic_keystems, reflex_adpt_keystems )))

colnames(descrip_comb)[apply(descrip_comb,1,which.max)]
descrip_comb_mini$result = descrip_comb_mini %>% names(descrip_comb_mini)[apply(descrip_comb_mini, 1, which.max)]



#colnames(descrip_comb)[max.col(descrip_comb, ties.method = "first")]
descrip_comb %>% 
  pivot_longer(cols = setdiff(names(.), "Category"), names_to = "column") %>% 
  filter(max(value) == value) %>% 
  pull(column)

descrip_comb_mini <- descrip_comb %>% select(-titles_acad,-id,-weblink_acad, -description, -key_category, -weblink, -titles)



summarize(descrip_comb_mini$result)

#other attempts
#descrip_comb_mini$max.col <- descrip_comb_mini[mutate(max.col = #colnames(descrip_comb_mini)[max.col(descrip_comb_mini, ties.method = c("first"))]) ]

#%>% mutate(max.col_2 = colnames(descrip_comb_mini)[max.col(descrip_comb_mini, ties.method = c("last"))])

descrip_comb_mini 
descrip_comb

descrip_comb_mini %>%
 summarize(result)
au_category_clean 

```


```{r}

descrip_comb
authors_list

authors_list <- read_csv("exports/authors_list_A_comb.csv")
authors_list_lean <- authors_list %>% select(titles, authors, category) 

authors_list_lean
#authors_list_lean <- unique(authors_list_lean)

#combine all authors into the courses
authors_list_lean <- authors_list_lean %>%
 group_by(titles) %>%
 summarize(category = str_c(category, collapse = ", "))
authors_list_lean


#
descrip_comb$titles <- descrip_comb$titles_acad
descrip_comb$titles <- descrip_comb$titles_acad

#trim whitespace from each side
descrip_comb$titles <- trimws(descrip_comb$titles)



authors_list_lean$titles <- trimws(authors_list_lean$titles)


#join the datasets
descrip_comb_test <- left_join(descrip_comb, authors_list_lean,  by = "titles", all=TRUE) 



descrip_comb_test <- descrip_comb_test %>% select(titles, participation_keystems, policy_coh_keystems, democratic_keystems, reflex_adpt_keystems, category)



#to long data
descrip_comb_test <- descrip_comb_test %>% 
  mutate(category = strsplit(as.character(category), ",")) %>%
  unnest(category)

#remove duplicates
descrip_comb_test <- unique(descrip_comb_test)
descrip_comb_test


#combine all categories into the courses
descrip_comb_test <- descrip_comb_test %>%
 group_by(titles) %>%
 summarize(category = str_c(category, collapse = ", "), participation_keystems = participation_keystems, policy_coh_keystems = policy_coh_keystems, democratic_keystems = democratic_keystems, reflex_adpt_keystems = reflex_adpt_keystems)

#capacity typology based on coding


descrip_comb_test$capacity[descrip_comb_test$participation_keystems>"0"] <- "participation"
descrip_comb_test$capacity[descrip_comb_test$policy_coh_keystems>"0"] <- "policy cohesion"
descrip_comb_test$capacity[descrip_comb_test$democratic_keystems>"0"] <- "democratic"
descrip_comb_test$capacity[descrip_comb_test$reflex_adpt_keystems>"0"] <- "reflexive adaptive"

#view
#descrip_comb_test <- unique(descrip_comb_test)

descrip_comb_test <- descrip_comb_test %>% 
  mutate(category = strsplit(as.character(category), ",")) %>%
  unnest(category)

descrip_comb_test


```



```{r}


write.csv(descrip_comb,"C:/Users/caitl/Documents/GitHub/mpp_thesis/exports/descrip_comb.csv", row.names = TRUE)


```



###combining with authors



```{r}
au_category_clean <- read_csv("exports/au_category_clean.csv")
descrip_comb <- descrip_comb %>% mutate(titles = titles_acad)



descrip_comb


```


```{r}
au_category_clean <- au_category %>%
 group_by(id) %>%
 summarize(category = str_c(category, collapse = ", "))
au_category_clean 


#separate type text from category
#au_category_clean <- str_split_fixed(au_category_clean$category, "Type,", 2)

au_category_clean <- separate(data = au_category_clean, col = category, into = c("category", "text"), sep = "Type,")

au_category_clean <- separate(data = au_category_clean, col = text, into = c("text", "remainder"), sep = ",")

au_category_clean

#remove redundant cols
au_category_clean <- mutate(au_category_clean, category=NULL, remainder=NULL)

au_category_clean$category <- au_category_clean$text

au_category_clean <- mutate(au_category_clean, text=NULL)

au_category_clean

#reorder
au_category_clean$id <- as.numeric(au_category_clean$id)

au_category_clean[order(au_category_clean$id, decreasing = FALSE),] 

authors_list
authors_list$id <- as.numeric(authors_list$id)




```




```{r}
#probably need later?
authors_corpus <- VCorpus(VectorSource(authors))
# Extra whitespace is eliminated by:
authors_corpus <- tm_map(authors_corpus, stripWhitespace)
# Conversion to lower case by:
authors_corpus <- tm_map(authors_corpus, content_transformer(tolower))
#Removal of stopwords by:
authors_corpus <- tm_map(authors_corpus, removeWords, stopwords("english"))
#Stemming is done by:
tm_map(authors_corpus, stemDocument)

 dtm <- DocumentTermMatrix(authors_corpus)
 inspect(dtm)



```





```{r}
chart_df <- data.frame(titles, authors, tags, SDGs, weblink)

knitr::kable(chart_df  %>% head(10))

chart_df <- tibble::rowid_to_column(chart_df, "id")

```



```{r}

#unsdglearn_subpage <- read_html("https://www.unsdglearn.org/courses/unido-industrial-analytics-platform-iap/")
```

```{r}
#old code
#keywords_nodes <- html_nodes(unsdglearn_subpage, 
#                           xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", #"keyword", " " ))]')
#keywords <- html_text(keywords_nodes)
#keywords


#audience_nodes <- html_nodes(unsdglearn_subpage, 
#                           xpath = '//*[(@id = "content")]//li')
#audience <- html_text(audience_nodes)
#audience


```
```{r}

# Define the KEYWORDS worker function
scraper <- function(weblink) {
  read_html(weblink) %>% 
    html_nodes(xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "keyword", " " ))]') %>% 
    html_text() %>% 
    enframe("id", "keywords")
}

# Iterate over the urls, applying the function each time
keywords <- map_dfr(weblink, scraper, .id = "id")

```


```{r}

# Define the AUDIENCE worker function
scraper <- function(weblink) {
  read_html(weblink) %>% 
    html_nodes(xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "resource-contentblock", " " )) and (((count(preceding-sibling::*) + 1) = 3) and parent::*)]//*[contains(concat( " ", @class, " " ), concat( " ", "wrapper", " " ))]') %>% 
    html_text() %>% 
    enframe("id", "audience")
}

# Iterate over the urls, applying the function each time
audience <- map_dfr(weblink, scraper, .id = "id")
audience


```


```{r}

#Combine into single columns

keywords_combined <- keywords %>%
 group_by(id) %>%
 summarize(keywords = str_c(keywords, collapse = ", "))

#df3 <- merge(audience, keywords_combined, by = "id")

chart_df <- merge(chart_df, keywords_combined, by = "id")
combined <- merge(chart_df, audience, by = "id", all = TRUE)

write.csv(combined,"C:/Users/caitl/Documents/GitHub/thesis/exports/unsdglearn.csv", row.names = TRUE)


```

```{r}

authors_list <- as.data.frame(authors_list)

write.csv(authors_list,"C:/Users/caitl/Documents/GitHub/mpp_thesis/exports/authors_list.csv", row.names = TRUE)

au_category_clean

au_category_list <- as.data.frame(au_category_clean)

write.csv(au_category_list,"C:/Users/caitl/Documents/GitHub/mpp_thesis/exports/au_category_list.csv", row.names = TRUE)


write.csv(results_urls,"C:/Users/caitl/Documents/GitHub/mpp_thesis/exports/results_urls.csv", row.names = TRUE)

write.csv(au_category_clean,"C:/Users/caitl/Documents/GitHub/mpp_thesis/exports/au_category_clean.csv", row.names = TRUE)


write.csv(authors_list_A,"C:/Users/caitl/Documents/GitHub/mpp_thesis/exports/authors_list_A.csv", row.names = TRUE)

```

```{r}
#long way
#------------------------UN PROGRAMS

un_programs <- c('UN-Habitat|UNDP|UNEP|UN Global Compact')

df <- data.frame(unique(authors_list_A$authors), un_programs = 0)
colnames(df)[1] <- "authors"
df[grep(un_programs, df$authors, value = F), "un_programs"] <- 1
df <- df[order(df$un_programs, decreasing = TRUE),] %>% unique()

df

#------------------------UN SPECIALIZED AGENCIES
un_specialized_agencies <-
  c('FAO|Food and Agriculture Organization of the United Nations|ICAO|International Civil Aviation Organization|IFAD|International Fund for Agricultural Development|ILO|International Labour Organization|IMF|International Monetary Fund|IMO|International Maritime Organization|ITU|International Telecommunication Union|UNESCO|United Nations Educational Scientific and Cultural Organization|UNIDO|United Nations Industrial Development Organization|UNWTO|World Tourism Organization|UPU|Universal Postal Union|WHO|World Health Organization|WIPO|World Intellectual Property Organization|WMO|World Meteorological Organization|World Bank Group|World Bank')
df_agent <- data.frame(authors_list_A$authors, un_specialized_agencies = 0)
colnames(df_agent)[1] <- "authors"
df_agent[grep(pattern = un_specialized_agencies, df_agent$authors, value = F), "un_specialized_agencies"] <- 1
df_agent <- df_agent[order(df_agent$un_specialized_agencies, decreasing = TRUE),] %>% unique()

df_agent

#------------------------UN
un <-
  c('UN DESA|UN Statistics Division|United Nations Statistics Division|United Nations Global Compact Academy|UNDCO|UNGCA|UNITAR')
df_un <- data.frame(authors_list_A$authors, un = 0)
colnames(df_un)[1] <- "authors"
df_un[grep(un, df_un$authors, value = F), "un"] <- 1
df_un <- df_un[order(df_un$un, decreasing = TRUE),] %>% unique()
df_un

#sapply(un, function(x) df_un$authors[grepl(x, df_un$authors)])

#sapply(un, function(x) df_un[grepl(x, un)])



#------------------------NONProfit
#space clear rm(unsdglearn,unsdglearn_silos,coursera)
ngo <-
  c('501','nonprofit','non-profit','non-governmental','nongovernmental','Non-governmental organization','Non-governmental','NGO','ngo','ingo','nongovernmental','philanthropic initiative')
df_ngo <- data.frame(authors_list_A$category, ngo = 0)
colnames(df_ngo)[1] <- "category"
df_ngo[grep(pattern = ngo, df_ngo$category, value = F), "ngo"] <- 1
df_ngo <- df_ngo[order(df_ngo$ngo, decreasing = TRUE),] %>% unique()
df_ngo

#sapply(ngo, function(x) df_ngo$authors[grepl(x, df_ngo$authors)])


#------------------



#------------------ If Coded, Change Category
#df
#df_agent 262
#authors_list_A
#add on new columns
#522
#authors_list_A
#df

```

```{r}
#toolbox
#soft_code$id <- seq.int(nrow(soft_code))

```
```{r}
library(rvest)
library(stringr)
library(jsonlite)
library(purrr)

descrip_comb



coursera <- read_csv("exports/coursera_c.csv")
unsdglearn_silos <- read_csv("exports/unsdglearn_silos.csv")
unsdglearn_systems <- read_csv("exports/unsdglearn_systems.csv")
unsdglearn <- read_csv("exports/unsdglearn.csv")
sdgacademy <- read_csv("exports/sdgacademy.csv")
unpan <- read_csv("exports/unpan.csv")
unssc <- read_csv("exports/unssc.csv")
unfccc <- read_csv("exports/unfccc.csv")
unfccc


weblink<- coursera$weblinks_coursera
# Define the DATE worker function
scraper <- function(weblink) {
  read_html(weblink) %>% 
   html_text() 
data <- jsonlite::parse_json(str_match_all(p,'resultListModel: (.*\\})')[[1]][,2])
results <- data$searchResponseModel$resultlist.resultlist$resultlistEntries[[1]]$resultlistEntry %>% 
    enframe("id", "date")
}

```
```{r}
# Iterate

df <- map_df(results, function(item) {

  data.frame(property = item$resultlist.realEstate$address$description$text,
             datePublished = item$`@publishDate`,
             stringsAsFactors=FALSE)
})



# Iterate over the urls, applying the function each time
audience <- map_dfr(weblink, scraper, .id = "id")
audience



p <- read_html('https://www.immobilienscout24.de/Suche/S-2/Wohnung-Miete/Rheinland-Pfalz/Koblenz') %>% html_text()
data <- jsonlite::parse_json(str_match_all(p,'resultListModel: (.*\\})')[[1]][,2])
results <- data$searchResponseModel$resultlist.resultlist$resultlistEntries[[1]]$resultlistEntry

df <- map_df(results, function(item) {

  data.frame(property = item$resultlist.realEstate$address$description$text,
             datePublished = item$`@publishDate`,
             stringsAsFactors=FALSE)
})

View(df)

```



```{r}

try <- left_join(descrip_comb, au_category_clean, by= "titles")

try <- try %>% select (titles, max.col, authors, au_category)
quick_sum <- try %>%
  group_by(au_category) %>%
  count(max.col)

try 
write.csv(try,"C:/Users/caitl/Documents/GitHub/mpp_thesis/exports/quick_sum.csv", row.names = TRUE)
```



```{r}




```

```{r}
#export file to fix the missing authors

write.csv(au_diff_coding_export,"C:/Users/caitl/Documents/GitHub/mpp_thesis/exports/au_diff_coding_export.csv", row.names = TRUE)

au_diff_coding_missing <- read_csv("data/au_diff_coding_export_fixed.csv")
au_diff_coding_main <- read_csv("data/au_diff_coding_export_main.csv")

colnames(au_diff_coding_missing)[1] <- "id"
colnames(au_diff_coding_main)[1] <- "id"

au_diff_coding_missing <- au_diff_coding_missing %>% select(id,titles,au_category)

#combine back together
au_diff_coding <- rbind(au_diff_coding_main, au_diff_coding_missing)
au_diff_coding
#fix/rename id col
colnames(au_diff_coding)[1] <- "id"
au_diff_coding <- au_diff_coding[order(au_diff_coding$id, decreasing = FALSE),]
au_diff_coding


au_diff_coding$titles <- iconv(au_diff_coding$titles, 'utf-8', 'ascii', sub = '')

au_diff_coding$titles <- tolower(au_diff_coding$titles)
au_diff_coding$titles <- gsub('sustainable development goals','sdgs',au_diff_coding$titles)
```
```{r}

#adding in softcode
soft_code <- read_csv("data/soft_code_auths_coded.csv")
#fix titles
soft_code$titles <- tolower(soft_code$titles)
soft_code$titles <- trimws(soft_code$titles)

au_diff_coding <- au_diff_coding %>%
 group_by(titles) %>%
 summarize(au_category = str_c(au_category, collapse = ", "))

soft_code_au <- soft_code %>% select(titles, au_category)

all_courses_au_coded <- merge(au_diff_coding,soft_code_au, by="titles", all=TRUE)

all_courses_au_coded$au_category <- paste(all_courses_au_coded$au_category.x,',',all_courses_au_coded$au_category.y)


write.csv(all_courses_au_coded,"C:/Users/caitl/Documents/GitHub/mpp_thesis/exports/all_courses_au_coded.csv", row.names = TRUE)

#you are here, need to combine the two lists in an intelligent way that still allows for messing with the dictionaries

```

```{r}
#in excel I added in unpan, unssc, unfccc
all_courses_au_coded %>% select(all_courses_au_coded$au_category == "un | un,un | un,un,un | un,un,un,un")

#in excel I separated out the UN ONLY courses
au_diff_un_only <- read_csv("data/au_diff_coding_un_only.csv")
au_diff_main <- read_csv("data/au_diff_coding_removed_un_only.csv")

#there are some NAs - come back to figure out why
au_diff_un_only

au_diff_un_only <- left_join(au_diff_un_only, descrip_comb, by= "titles", x.all = "TRUE")
au_diff_main <- left_join(au_diff_main, descrip_comb, by= "titles", x.all = "TRUE")

au_diff_un_only


#how many of each
table(au_diff_un_only$max.col)

au_matrix_un <- table(au_diff_un_only$max.col)
au_matrix_un

au_matrix_un <- as.data.frame(au_matrix_un)
au_matrix_un


#add in un only type
au_matrix_un$Var2 <- "un only"
au_matrix_un

```



```{r}
final_coded_dataset <- read_csv("data/final_coded_dataset.csv")
all_courses_coded_adendum<- left_join(final_coded_dataset,descrip_comb, by = "titles", all.x=TRUE) 
all_courses_coded

all_courses_coded_adendum

unsdglearn
unsdglearn$source <- "unsdglearn"
sdgacademy$source <- "sdgacad" 



unsdglearn$titles <- iconv(unsdglearn$titles, 'utf-8', 'ascii', sub = '')

unsdglearn$titles <- tolower(unsdglearn$titles)
unsdglearn$titles <- gsub('sustainable development goals','sdgs',unsdglearn$titles)


sdgacademy$titles <- iconv(sdgacademy$titles, 'utf-8', 'ascii', sub = '')

sdgacademy$titles <- tolower(sdgacademy$titles)
sdgacademy$titles <- gsub('sustainable development goals','sdgs',sdgacademy$titles)

tags <- unsdglearn %>% select(titles, source)
tags_2 <- sdgacademy %>% select(titles, source)

tags <- rbind(tags, tags_2)

all_courses_coded_adendum<- left_join(all_courses_coded_adendum,tags, by = "titles", all.x=TRUE)

write.csv(all_courses_coded_adendum,"C:/Users/caitl/Documents/GitHub/mpp_thesis/exports/unsdglearn_n_acad_coded.csv", row.names = TRUE)

```