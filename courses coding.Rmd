---
title: "Web Scraping"
subtitle: "Author Standardization"
author: "Caitlin Sarro"
date: "2/9/2022"
output: 
  html_document:
    toc: TRUE
    df_print: paged
    number_sections: FALSE
    highlight: tango
    theme: lumen
    toc_depth: 3
    toc_float: true
    css: custom.css 
    self_contained: false
    includes:
      after_body: footer.html
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```
***


**Step 1.** Load the packages `rvest` and `stringr`.

```{r, message=F}
#install.packages("")
library(RSelenium)
library(tidyverse)
library(rvest)
library(stringr)
library(readr)

library(reshape2)
library(dplyr)

library(xml2)
library(purrr)
library(tibble)

library(data.table)

library(tm)
library(dplyr)
library(tidytext)
library(ggplot2)

library(RCurl)

library(quanteda)

#for categorization
library(quanteda)
library(tidytext)
#library(quanteda.textmodels)
#library(caret)
library(SnowballC)

library(topicmodels)
library(quanteda.dictionaries)

#plots
library(GGally)

```



```{r}
#Capacity Building https://www.unsdglearn.org/courses/?_sf_s=capacity%20building
#Breaking the Silos https://www.unsdglearn.org/courses/?_sfm_sdg=5

coursera <- read_csv("exports/coursera_c.csv")
unsdglearn_silos <- read_csv("exports/unsdglearn_silos.csv")
unsdglearn <- read_csv("exports/unsdglearn.csv")
sdgacademy <- read_csv("exports/sdgacademy.csv")
unpan <- read_csv("exports/unpan.csv")
unssc <- read_csv("exports/unssc.csv")
unfccc <- read_csv("exports/unfccc.csv")
unfccc
#quick fixes
coursera$description <- coursera$keywords
sdgacademy$titles <- sdgacademy$titles_acad
unpan$titles <- unpan$titles_unpan
unssc$titles <- unssc$titles_unssc
unfccc$titles <- unfccc$titles_unfccc

keywords_cour <- coursera %>% select(titles, description)
keywords_unsdglearn_silos <- unsdglearn_silos %>% select(titles, description) 
keywords_unsdglearn <- unsdglearn %>% select(titles, description) 
keywords_sdgacademy <- sdgacademy %>% select(titles, description)
keywords_unpan <- unpan %>% select(titles, description)
keywords_unssc <- unssc %>% select(titles, description)
keywords_unfccc <- unfccc %>% select(titles,description)
# descrp_acad


#combine into one set
combined <- rbind(keywords_cour, keywords_unsdglearn_silos, keywords_unsdglearn, keywords_sdgacademy, keywords_unpan, keywords_unssc)

```

```{r}
#remove words that could skew analysis 


combined <- combined %>%
    mutate(description = str_remove_all(description, "participants"))

#use | to add more

#export to fix translation problem
write.csv(combined,"C:/Users/caitl/Documents/GitHub/mpp_thesis/exports/combined_for_coding.csv", row.names = TRUE)

#import back in after translation
combined <- read_csv("data/combined_for_coding.csv")

```



```{r}
#to long data
# combined <- sdgacad_df_mini

#combined <- combined %>% 
#  mutate(keywords = strsplit(as.character(keywords), ",")) %>%
#  unnest(keywords)

#trim leading and ending whitespaces
#combined$keywords <- trimws(combined$keywords)

descrp_list <- combined
#combined

#make a dataframe
#duplicate column (safety redudancy)
descrp_list$key_category <- descrp_list$description


#text cleaning punctuation
descrp_list$key_category <- gsub('[[:punct:] ]+',' ',descrp_list$key_category)
#text cleaning lowercase
descrp_list$key_category <- tolower(descrp_list$key_category)

#text cleaning symbols
descrp_list$key_category <- gsub('“',' ',descrp_list$key_category)
descrp_list$key_category <- gsub('”',' ',descrp_list$key_category)
descrp_list$key_category <- gsub('‘',' ',descrp_list$key_category)
descrp_list$key_category <- gsub('’',' ',descrp_list$key_category)
descrp_list$key_category <- gsub('–',' ',descrp_list$key_category)
#remove stopwords
descrp_list$key_category = removeWords(descrp_list$key_category, stopwords("english"))
#remove whitespace
descrp_list$key_category = stripWhitespace(descrp_list$key_category)
descrp_list$key_category <- trimws(descrp_list$key_category)
descrp_list$key_category <- gsub('  ',' ',descrp_list$key_category)

dfmat_descrip <- dfm(tokens(descrp_list$key_category, remove_punct = TRUE))

head(descrp_list$key_category)
head(descrp_list)

```

```{r}
#test using a prefab dictionary

dict <- dictionary(file = "data/dictionary/policy_agendas_english.lcd")

#dfm(dfmat1, remove = stopwords("english"), stem = TRUE, verbose = TRUE)

dfmat_descrip <- dfm(tokens(descrp_list$key_category, remove_punct = TRUE, tolower = TRUE, stem = TRUE, verbose = TRUE))
dfm_wordstem(dfmat_descrip)
#dfmat_descrip <-dfm(dfmat_descrip, remove_punct = TRUE, tolower = TRUE, stem = TRUE, verbose = TRUE)

d_descrip <- dfm_lookup(dfmat_descrip, dict)
d_descrip <- as.data.frame(as.matrix(d_descrip))
d_descrip$id <- seq.int(nrow(d_descrip))

d_descrip


```

```{r}
#import text from  Glass, L., & Newig, J. (2019). Governance for achieving the sustainable development goals: How important are participation, policy coherence, reflexivity, adaptation and democratic institutions? Earth System Governance, 2, 100031. doi:10.1016/j.esg.2019.100031

#-------PARTICIPATION
participation <- read_csv("data/dictionary/glass newig/participation.csv")

tidy_participation<- participation %>%
    unnest_tokens("word", Terms)

#remove stopwords
data("stop_words")
    tidy_participation<-tidy_participation %>%
      anti_join(stop_words)
    
#remove punctuation
#tidy_participation<-tidy_participation[-grep("\\b\\d+\\b", tidy_participation$word),]
tidy_participation$word<- gsub("[[:punct:]]|[[:digit:]]|^http:\\/\\/.*|^https:\\/\\/.*","",tidy_participation$word)

#to lowercase
tidy_participation$word<- tolower(tidy_participation$word)

#remove whitespace
tidy_participation$word <- gsub("\\s+","",tidy_participation$word)

#stemming the keywords
tidy_participation<-tidy_participation %>%
      mutate_at("word", funs(wordStem((.), language="en")))


#see most frequent   
#tidy_participation %>%
#  count(word) %>%
#    arrange(desc(n))

#keep unique values
tidy_participation <- unique(tidy_participation)

policy_coherence <- read_csv("data/dictionary/glass newig/policy coherence.csv")
reflexivity_and_adaptation <- read_csv("data/dictionary/glass newig/reflexivity and adaptation.csv")

#terms into a character list
participation_keystems <- tidy_participation$word
participation_keystems <- as.list(tidy_participation$word)


participation_keystems <- as.list(tidy_participation)
names(participation_keystems)
participation_keystems

```








```{r}
#import text from  Glass, L., & Newig, J. (2019). Governance for achieving the sustainable development goals: How important are participation, policy coherence, reflexivity, adaptation and democratic institutions? Earth System Governance, 2, 100031. doi:10.1016/j.esg.2019.100031

#-------POLICY COHERENCE
policy_coherence <- read_csv("data/dictionary/glass newig/policy coherence.csv")

tidy_policy_c<- policy_coherence %>%
    unnest_tokens("word", Terms)

#remove stopwords
data("stop_words")
    tidy_policy_c<-tidy_policy_c %>%
      anti_join(stop_words)
    
#remove punctuation
#tidy_participation<-tidy_participation[-grep("\\b\\d+\\b", tidy_participation$word),]
tidy_policy_c$word<- gsub("[[:punct:]]|[[:digit:]]|^http:\\/\\/.*|^https:\\/\\/.*","",tidy_policy_c$word)

#to lowercase
tidy_policy_c$word<- tolower(tidy_policy_c$word)

#remove whitespace
tidy_policy_c$word <- gsub("\\s+","",tidy_policy_c$word)

#stemming the keywords
tidy_policy_c<-tidy_policy_c %>%
      mutate_at("word", funs(wordStem((.), language="en")))


#see most frequent   
#tidy_participation %>%
#  count(word) %>%
#    arrange(desc(n))

#keep unique values
tidy_policy_c <- unique(tidy_policy_c)
tidy_policy_c


#terms into a character list
policy_coh_keystems <- tidy_policy_c$word #pick one
policy_coh_keystems <- as.list(tidy_policy_c)
names(policy_coh_keystems)
policy_coh_keystems

policy_coh_keystems <- as.list(tidy_policy_c)
names(policy_coh_keystems)
policy_coh_keystems


```


```{r}
#import text from  Glass, L., & Newig, J. (2019). Governance for achieving the sustainable development goals: How important are participation, policy coherence, reflexivity, adaptation and democratic institutions? Earth System Governance, 2, 100031. doi:10.1016/j.esg.2019.100031

#-------DEMOCRATIC INSTITUTIONS
democratic_institutions <- read_csv("data/dictionary/glass newig/democratic institutions .csv")

tidy_democratic <- democratic_institutions %>%
    unnest_tokens("word", Terms)

#remove stopwords
data("stop_words")
    tidy_democratic <- tidy_democratic %>%
      anti_join(stop_words)
    
#remove punctuation
#tidy_participation<-tidy_participation[-grep("\\b\\d+\\b", tidy_participation$word),]
tidy_democratic$word <- gsub("[[:punct:]]|[[:digit:]]|^http:\\/\\/.*|^https:\\/\\/.*","", tidy_democratic$word)

#to lowercase
tidy_democratic$word<- tolower(tidy_democratic$word)

#remove whitespace
tidy_democratic$word <- gsub("\\s+","",tidy_democratic$word)

#stemming the keywords
tidy_democratic <- tidy_democratic %>%
      mutate_at("word", funs(wordStem((.), language="en")))


#see most frequent   
#tidy_participation %>%
#  count(word) %>%
#    arrange(desc(n))

#keep unique values
tidy_democratic <- unique(tidy_democratic)

policy_coherence <- read_csv("data/dictionary/glass newig/policy coherence.csv")
reflexivity_and_adaptation <- read_csv("data/dictionary/glass newig/reflexivity and adaptation.csv")




#terms into a character list
democratic_keystems <- tidy_democratic$word
democratic_keystems <- as.list(tidy_democratic$word)


democratic_keystems <- as.list(tidy_democratic)
names(democratic_keystems)
democratic_keystems
```





```{r}
#import text from  Glass, L., & Newig, J. (2019). Governance for achieving the sustainable development goals: How important are participation, policy coherence, reflexivity, adaptation and democratic institutions? Earth System Governance, 2, 100031. doi:10.1016/j.esg.2019.100031

#-------REFLEXIVITY AND ADAPTATION INSTITUTIONS
reflexivity_and_adaptation <- read_csv("data/dictionary/glass newig/reflexivity and adaptation.csv")

tidy_reflex_adapt <- reflexivity_and_adaptation %>%
    unnest_tokens("word", Terms)

#remove stopwords
data("stop_words")
    tidy_reflex_adapt <- tidy_reflex_adapt %>%
      anti_join(stop_words)
    
#remove punctuation
#tidy_participation<-tidy_participation[-grep("\\b\\d+\\b", tidy_participation$word),]
tidy_reflex_adapt$word <- gsub("[[:punct:]]|[[:digit:]]|^http:\\/\\/.*|^https:\\/\\/.*","", tidy_reflex_adapt$word)

#to lowercase
tidy_reflex_adapt$word<- tolower(tidy_reflex_adapt$word)

#remove whitespace
tidy_reflex_adapt$word <- gsub("\\s+","",tidy_reflex_adapt$word)

#stemming the keywords
tidy_reflex_adapt <- tidy_reflex_adapt %>%
      mutate_at("word", funs(wordStem((.), language="en")))


#see most frequent   
#tidy_participation %>%
#  count(word) %>%
#    arrange(desc(n))

#keep unique values
tidy_reflex_adapt <- unique(tidy_reflex_adapt)



#terms into a character list
reflex_adpt_keystems <- tidy_reflex_adapt$word
reflex_adpt_keystems <- as.list(tidy_reflex_adapt$word)


reflex_adpt_keystems <- as.list(tidy_reflex_adapt)
names(reflex_adpt_keystems)
reflex_adpt_keystems

wordStem("participatory")

```

```{r}
#All in one dictionary
dict_descrip <- dictionary(list(participation_keystems = c("particip", "propos","assess","relev", "associ","compet","represent","stakehold", "collabor","consensus","decis", "facilit", "aspect","form",  "concept", "activ", "matter","indirect","affair","vote", "extend","engag","stakeholder engag", "multi-stakehold", "cooper", "divers inclus","vulner popul", "cultur heritag", "indigen", "youth indigen", "reduc inequ",  "build trust",  "stakehold priorit",   "social inclus", "cultur divers", "discuss forum","convers", "interact platform", "live heritag", "interdisciplinari issu", "interdisciplinari", "delib", "process", "citizen", "repres","deliber", "collect", "panel", "institution",  "juri", "trust", "participatori","participatori process", "involv","communiti", "societi", "ballot","dialogu", "discuss", "inclus","youth", "assembl", "confer","facilit method","communic","trainer", "confid","emot",  "listen","reflect", "relationship", "rapport", "deliveri","negoti", "conflict","techniqu","deleg", "expect", "behaviour","behavior","creativ", "frontlin", "indigen", "union", "disabl","elder", "marginla","marginlais"  
),
                             policy_coh_keystems = c("coher", "extent", "structur", "foster", "coordin", "implement", "interministeri", "systemat", "mutual"  ,"reinforc" ,  "synergi"    , "integr", "overcom",             "silo",                 "unintend consequ" ,    "diverg", "interrel" , "trade off" ,           "feedback loop" ,       "institut", "level"  , "strategi" ,            "horizont framework" ,           "practic", "vertic" ,              "align"   ,             "approach" ,            "applic", "balanc"  ,             "conceptu" ,            "link"  ,               "operationalis", "operation" ,           "prioriti"  ,           "subnat"  ,             "aspir"  ,  "mechan"    ,           "partnership",          "complement",           "connect",  "contradictori",        "leverage point" ,      "integr plan" ,         "depart", "ensur"   ,             "comprehens system"     ,          "comprehens",           "consist", "duplic" ,              "exist"   ,             "interlink",            "lever" ,  "relat" ,               "consensus"  ,          "commit",               "intern" , "reiter",               "treati"  ,             "contradict",           "reaffirm" , "shift" ,               "symbiosi" ,            "reciproc",             "solidar"  , "cooper"    ,           "polici coher" ,        "align polici decis",   "polici sequenc", "polici align" ,        "interconnect" ,        "increas coher",        "across govern depart", "institut structur" ,   "common goal" ,         "complex issu" ,        "govern depart", "transgovernance", "integrated plan"),
                             
                             
                               democratic_keystems = c("elector", "media", "freedom", "civil", "liberti", "rule", "law", "govern", "transpar", "percept", "credibl", "corrupt", "express", "fair", "legislatur", "checks bal", "altern", "respect", "independ", "judiciari", "legitim", "liber", "diplomaci", "democrat", "democraci" , "voter" , "franchis"  , "pluralist" ,  "competit" , "equal" , "autocraci" , "regim"   ,"repres","peac","institut" ,"inclus process", "equal access" , "govern regul", "good govern", "polit" ,"account"  ,"effici govern"   ,"effect govern"   ,"countri"  ,         "collect action"  ,         "reduct"  ,          "transact" ,"public offic"  ,          "free elect"    ,           "inclus"    ,         "universal right"     ,         "franchi"  ,         "citizen"     , "civil liberti" ,    "democrat institut", "negoti skill" ,"ethic leadership",  "contract negoti",   "govern negoti" ,    "transact cost" ,    "human right"   ,"free elect", "polit philosoph"  , "polit polar"  ,       "repres democraci"),

                             
                                
                             
                               reflex_adpt_keystems = c("reflex", "monitor","reform","regulatori","instrument", "trajectori","perform","transdisciplinari", "process improv","self monitor", "strateg plan","impact assess","organiz reform","adapt", "evidence-bas", "self-awar", "develop strategi", "long term", "assess","data", "measur","account","system chang","structur chang","structur shift" , "polici instrument","inform manag","system think","python","help compani", "scienc base", "program polici","metric","cycl",  "strategic plan" ,"human resourc","guidelin",  "integr","daily practic","public servic", "long term",  "development plan", "domest","evidence bas", "report", "progress",  "evalu", "statist","indic", "impact", "evid",  "reflect","result","analysi","actor", "me","collect","criteria") 
   ))









dup <-data.frame(reflex_adpt_keystems)
dup <- unique(dup)
dup$reflex_adpt_keystems


    




```

```{r}

#run dictionary against descriptions
#stemDocument(last_stem)


descrp_list <- descrp_list %>%
  mutate(stem = stemDocument(descrp_list$key_category))


dfmat_descrip <- dfm(tokens(descrp_list$stem, remove_punct = TRUE) %>% 
  tokens_ngrams(1:2))
d_descrip_ref <- dfm_lookup(dfmat_descrip, dict_descrip,  valuetype = c("glob"))
d_descrip_ref <- as.data.frame(as.matrix(d_descrip_ref))
d_descrip_ref$id <- seq.int(nrow(d_descrip_ref))

d_descrip_ref

descrp_list
######


library(tokenizers)
library(stopwords)
library(tidyverse)
words_as_tokens <- setNames(lapply(sapply(descrp_list$description, 
                                 tokenize_words, 
                                 stopwords = stopwords(language = "en", source = "smart")), 
                          function(x) as.data.frame(sort(table(x), TRUE), stringsAsFactors = F)), descrp_list$titles)

# tidyverse's job
df <- words_as_tokens %>%
  bind_rows(, .id = "id") %>%
  rename(word = x)

# output

df %>% filter(Freq >= 10)

```



```{r}
#checking which words are the most matched per category 


toks <- tokens(tail(descrp_list$stem, remove_punct = TRUE)) %>% 
  tokens_ngrams(1:2)

dfm_list <- list()
for (key in names(dict_descrip)) {
  this_dfm <- tokens_select(toks, dict_descrip[key], pad = TRUE) %>%
    tokens_compound(dict_descrip[key]) %>%
    tokens_replace("", "OTHER") %>%
    dfm(tolower = FALSE)
  dfm_list <- c(dfm_list, this_dfm)
}
names(dfm_list) <- names(dict_descrip)
dfm_list

dfm_list_df <- as.data.frame(dfm_list)
```

```{r}

#d_descrip_demo
#d_descrip_part
#d_descrip_pol
#d_descrip_ref


#descrip_comb <- left_join(d_descrip_demo,d_descrip_part, by = "id") 
#descrip_comb <- left_join(descrip_comb,d_descrip_pol, by = "id") 
#descrip_comb <- left_join(descrip_comb,d_descrip_ref, by = "id")

#descrip_comb

#adding index as a col for matching
descrp_list$id <- seq.int(nrow(descrp_list))

#descrp_list <- sdgacad_df_mini
descrip_comb <- left_join(descrp_list,d_descrip_ref, by = "id") 

```

```{r}
#assigning a category label per course

descrip_comb$max.col <- descrip_comb %>% select(participation_keystems:reflex_adpt_keystems) %>% {names(.)[max.col(.)]}
descrip_comb

#how many of each
table(descrip_comb$max.col)

```
```{r}
#wider export (useful for checking)
write.csv(descrip_comb,"C:/Users/caitl/Documents/GitHub/mpp_thesis/exports/descrip_comb.csv", row.names = TRUE)
descrip_comb


```

```{r}
#I AM HEREEE
#check top keywords per topic
#Relative frequency analysis (keyness)
dfmat_descrip
names(dfm_list) <- names(dict_descrip)
dfm_list

dfm_list_df

dfmat_descrip_trim <- dfm_trim(dfmat_descrip, min_termfreq = 10, min_docfreq = 5)

dfmat_descrip_trim

docvars(dfmat_descrip)

dfmat_descrip
dict_descrip
dfm_list

tidy(dfm_list)
class(dfmat_descrip)

######

descrip_comb
top_words_partic <- filter(descrip_comb, max.col == "participation_keystems") 
top_words_pol_co <- filter(descrip_comb, max.col == "policy_coh_keystems") 
top_words_demo <- filter(descrip_comb, max.col == "democratic_keystems") 
top_words_reflex <- filter(descrip_comb, max.col == "reflex_adpt_keystems") 


descrip_comb

words_as_tokens <-  setNames(lapply(sapply(top_words_partic$description, 
                                 tokenize_words, 
                                 stopwords = stopwords(language = "en", source = "smart")), 
                          function(x) as.data.frame(sort(table(x), TRUE), stringsAsFactors = F)), top_words_partic$titles)

# tidyverse's job
df <- words_as_tokens %>%
  bind_rows(, .id = "id") %>%
  rename(word = x)

# output

df %>% filter(Freq >= 8) %>% arrange(desc(Freq))




```


```{r}
#check keywords in context



```




###start to combine with authors


```{r}
#try to combine with authors
#descrip_comb
#au_category_clean
au_category_clean <- read_csv("exports/au_category_clean.csv")

au_diff_coding <- left_join(descrip_comb, au_category_clean, by= "titles")
au_diff_coding <- au_diff_coding %>% select (titles, max.col, authors, au_category)


au_diff_coding

au_diff_coding_export <- au_diff_coding %>%
 group_by(titles) %>%
 summarize(au_category = str_c(au_category, collapse = ", "))
au_diff_coding_export

```

```{r}

try <- left_join(descrip_comb, au_category_clean, by= "titles")

try <- try %>% select (titles, max.col, authors, au_category)
quick_sum <- try %>%
  group_by(au_category) %>%
  count(max.col)


write.csv(quick_sum,"C:/Users/caitl/Documents/GitHub/mpp_thesis/exports/quick_sum.csv", row.names = TRUE)
```



```{r}




```

```{r}
#export file to fix the missing authors

write.csv(au_diff_coding_export,"C:/Users/caitl/Documents/GitHub/mpp_thesis/exports/au_diff_coding_export.csv", row.names = TRUE)

au_diff_coding_missing <- read_csv("data/au_diff_coding_export_fixed.csv")
au_diff_coding_main <- read_csv("data/au_diff_coding_export_main.csv")

#combine back together
au_diff_coding <- rbind(au_diff_coding_main, au_diff_coding_missing)
au_diff_coding
#fix/rename id col
colnames(au_diff_coding)[1] <- "id"
au_diff_coding <- au_diff_coding[order(au_diff_coding$id, decreasing = FALSE),]
au_diff_coding

write.csv(au_diff_coding,"C:/Users/caitl/Documents/GitHub/mpp_thesis/exports/au_diff_coding.csv", row.names = TRUE)
```

```{r}
#in excel I added in unpan, unssc, unfccc


#in excel I separated out the UN ONLY courses
au_diff_un_only <- read_csv("data/au_diff_coding_un_only.csv")
au_diff_main <- read_csv("data/au_diff_coding_removed_un_only.csv")

#there are some NAs - come back to figure out why
au_diff_un_only

au_diff_un_only <- left_join(au_diff_un_only, descrip_comb, by= "titles", x.all = "TRUE")
au_diff_main <- left_join(au_diff_main, descrip_comb, by= "titles", x.all = "TRUE")

au_diff_un_only


#how many of each
table(au_diff_un_only$max.col)

au_matrix_un <- table(au_diff_un_only$max.col)
au_matrix_un

au_matrix_un <- as.data.frame(au_matrix_un)
au_matrix_un


#add in un only type
au_matrix_un$Var2 <- "un only"
au_matrix
au_matrix_un

```

```{r}
table(au_diff_main$max.col)

au_diff_main
#to long data
au_diff_main_long <- au_diff_main %>% 
  mutate(au_category = strsplit(as.character(au_category), ",")) %>%
  unnest(au_category)


#remove white spaces
au_diff_main_long$au_category <- trimws(au_diff_main_long$au_category)
au_diff_main_long

write.csv(au_diff_main_long,"C:/Users/caitl/Documents/GitHub/mpp_thesis/exports/au_diff_main_long.csv", row.names = TRUE)

#remove duplicates
#au_diff_main_long <- unique(au_diff_main_long)

au_diff_main_long
table(au_diff_main_long$max.col)

au_diff_main_long

#summary table
au_matrix <- table(au_diff_main_long$max.col, au_diff_main_long$au_category)
#table to dataframe
au_matrix <- as.data.frame(au_matrix)
au_matrix


#change to characters
au_matrix$Var1 <- as.character(au_matrix$Var1)
au_matrix$Var2 <- as.character(au_matrix$Var2)

#simply categories
au_matrix$Var2[au_matrix$Var2 == "cultural institution"] <- "civil society"
au_matrix$Var2[au_matrix$Var2 == "ngo"] <- "civil society"
au_matrix$Var2[au_matrix$Var2 == "business"] <- "business/finance"
au_matrix$Var2[au_matrix$Var2 == "finance"] <- "business/finance"
au_matrix$Var2[au_matrix$Var2 == "un ngo"] <- "un"


#create facet groups
au_matrix$facet[au_matrix$Var2 == "private university"] <- "Gr1"
au_matrix$facet[au_matrix$Var2 == "public university"] <- "Gr1"
au_matrix$facet[au_matrix$Var2 == "business/finance"] <- "Gr2"
au_matrix$facet[au_matrix$Var2 == "research  policy institutes"] <- "Gr2"
au_matrix$facet[au_matrix$Var2 == "civil society"] <- "Gr3"
au_matrix$facet[au_matrix$Var2 == "government"] <- "Gr3"
au_matrix$facet[au_matrix$Var2 == "un"] <- "Gr4"

#sum up repeats
au_matrix <- au_matrix %>% 
  group_by(Var1, Var2, facet) %>%
  summarize_all(sum)

au_matrix

write.csv(au_matrix,"C:/Users/caitl/Documents/GitHub/mpp_thesis/exports/au_matrix.csv", row.names = TRUE)
au_matrix_simple <- read_csv("data/au_matrix_simplified.csv")
au_matrix_simple
#fix/rename cols
#au_matrix <- colnames(au_matrix$Var1)[1] <- "category"
#au_matrix <- colnames(au_matrix$Var2)[2] <- "author"

```



```{r}
library(ggplot2)
 
# create a dataset
specie <- c(rep("sorgho" , 3) , rep("poacee" , 3) , rep("banana" , 3) , rep("triticum" , 3) )
condition <- rep(c("normal" , "stress" , "Nitrogen") , 4)
value <- abs(rnorm(12 , 0 , 15))
data <- data.frame(specie,condition,value)
 
# Stacked
ggplot(au_matrix_simple, aes(fill=Var1, y=Freq, x=Var2)) + 
    geom_bar(position="stack", stat="identity")

au_matrix_simple
au_matrix



```

```{r}

ggplot(au_matrix, aes(fill=Var1, y=Freq, x=Var2)) + 
    geom_bar(position="stack", stat="identity")

# Quick display of two cabapilities of GGally, to assess the distribution and correlation of variables 

 
# Create data 
data <- data.frame( var1 = 1:100 + rnorm(100,sd=20), v2 = 1:100 + rnorm(100,sd=27), v3 = rep(1, 100) + rnorm(100, sd = 1)) 
data$v4 = data$var1 ** 2 
data$v5 = -(data$var1 ** 2) 
 
# Check correlations (as scatterplots), distribution and print corrleation coefficient 
ggpairs(au_matrix, title="correlogram with ggpairs()") 

ggcorr(au_matrix, method="circle")

au_matrix




line_unonly = lm(Var1 ~ Freq )'data= au_matrix_un, aes(x=Var1, y=Freq, group = Var1), color = "black"'

au_matrix
#do the plot
ggplot(au_matrix, aes(fill=Var2, y=Freq, x=Var1)) +
  geom_point(size=2, shape=23) +
  geom_line(aes(group = Var2), color="gray") +
  theme_minimal()  + facet_wrap( ~ facet) +
  geom_line(data = au_matrix_un, aes(x=Var1, y=Freq))
au_matrix


line2 = data.frame(x=xl,y=yl,type)

ggplot(data, aes(x,y)) + geom_line() + facet_wrap(~type) +
   geom_line(data = line_unonly)

au_matrix_un

###
au_matrix
#do the plot
ggplot(au_matrix, aes(fill=Var2, y=Freq, x=Var1)) +
  geom_point(size=2, shape=23) +
  geom_line(aes(group = Var2), color="gray") +
  theme_minimal()  + facet_wrap( ~ facet)  +
  geom_point(data = au_matrix_un, aes(x=Var1, y=Freq)) +
  geom_point(size=2, shape=23)
au_matrix


xl    = c( 4,  1)
yl    = c( 1,  4)
type =rep(LETTERS[1:4], each=2)
line2 = data.frame(x=au_matrix_un$Var1,y=au_matrix_un$Freq,au_matrix_un)

ggplot(data, aes(x,y)) + geom_line() + facet_wrap(~type) +
   geom_line(data = line2)
au_matrix_un

```


```{r}
#old code



descrip_comb$result = descrip_comb %>% select(participation_keystems, policy_coh_keystems, democratic_keystems, reflex_adpt_keystems) %>% names(descrip_comb)[apply(descrip_comb, 1, which.max)]

descrip_comb$result <- colnames(descrip_comb)[apply(max.col = colnames(pmax(participation_keystems, policy_coh_keystems, democratic_keystems, reflex_adpt_keystems), MARGIN = 4))]


descrip_comb_test <- descrip_comb %>% select( -id) %>% colnames(descrip_comb)[apply(descrip_comb,1,which.max)]

colnames(descrip_comb_mini)[apply(descrip_comb_mini,1,which.max)]

descrip_comb %>% 
  mutate(max.col = colnames(pmax(participation_keystems, policy_coh_keystems, democratic_keystems, reflex_adpt_keystems )))

colnames(descrip_comb)[apply(descrip_comb,1,which.max)]
descrip_comb_mini$result = descrip_comb_mini %>% names(descrip_comb_mini)[apply(descrip_comb_mini, 1, which.max)]



#colnames(descrip_comb)[max.col(descrip_comb, ties.method = "first")]
descrip_comb %>% 
  pivot_longer(cols = setdiff(names(.), "Category"), names_to = "column") %>% 
  filter(max(value) == value) %>% 
  pull(column)

descrip_comb_mini <- descrip_comb %>% select(-titles_acad,-id,-weblink_acad, -description, -key_category, -weblink, -titles)



summarize(descrip_comb_mini$result)

#other attempts
#descrip_comb_mini$max.col <- descrip_comb_mini[mutate(max.col = #colnames(descrip_comb_mini)[max.col(descrip_comb_mini, ties.method = c("first"))]) ]

#%>% mutate(max.col_2 = colnames(descrip_comb_mini)[max.col(descrip_comb_mini, ties.method = c("last"))])

descrip_comb_mini 
descrip_comb

descrip_comb_mini %>%
 summarize(result)
au_category_clean 

```


```{r}

descrip_comb
authors_list

authors_list <- read_csv("exports/authors_list_A_comb.csv")
authors_list_lean <- authors_list %>% select(titles, authors, category) 

authors_list_lean
#authors_list_lean <- unique(authors_list_lean)

#combine all authors into the courses
authors_list_lean <- authors_list_lean %>%
 group_by(titles) %>%
 summarize(category = str_c(category, collapse = ", "))
authors_list_lean


#
descrip_comb$titles <- descrip_comb$titles_acad
descrip_comb$titles <- descrip_comb$titles_acad

#trim whitespace from each side
descrip_comb$titles <- trimws(descrip_comb$titles)



authors_list_lean$titles <- trimws(authors_list_lean$titles)


#join the datasets
descrip_comb_test <- left_join(descrip_comb, authors_list_lean,  by = "titles", all=TRUE) 



descrip_comb_test <- descrip_comb_test %>% select(titles, participation_keystems, policy_coh_keystems, democratic_keystems, reflex_adpt_keystems, category)



#to long data
descrip_comb_test <- descrip_comb_test %>% 
  mutate(category = strsplit(as.character(category), ",")) %>%
  unnest(category)

#remove duplicates
descrip_comb_test <- unique(descrip_comb_test)
descrip_comb_test


#combine all categories into the courses
descrip_comb_test <- descrip_comb_test %>%
 group_by(titles) %>%
 summarize(category = str_c(category, collapse = ", "), participation_keystems = participation_keystems, policy_coh_keystems = policy_coh_keystems, democratic_keystems = democratic_keystems, reflex_adpt_keystems = reflex_adpt_keystems)

#capacity typology based on coding


descrip_comb_test$capacity[descrip_comb_test$participation_keystems>"0"] <- "participation"
descrip_comb_test$capacity[descrip_comb_test$policy_coh_keystems>"0"] <- "policy cohesion"
descrip_comb_test$capacity[descrip_comb_test$democratic_keystems>"0"] <- "democratic"
descrip_comb_test$capacity[descrip_comb_test$reflex_adpt_keystems>"0"] <- "reflexive adaptive"

#view
#descrip_comb_test <- unique(descrip_comb_test)

descrip_comb_test <- descrip_comb_test %>% 
  mutate(category = strsplit(as.character(category), ",")) %>%
  unnest(category)

descrip_comb_test


```



```{r}


write.csv(descrip_comb,"C:/Users/caitl/Documents/GitHub/mpp_thesis/exports/descrip_comb.csv", row.names = TRUE)


```



###combining with authors



```{r}
au_category_clean <- read_csv("exports/au_category_clean.csv")
descrip_comb <- descrip_comb %>% mutate(titles = titles_acad)



descrip_comb


```


```{r}
au_category_clean <- au_category %>%
 group_by(id) %>%
 summarize(category = str_c(category, collapse = ", "))
au_category_clean 


#separate type text from category
#au_category_clean <- str_split_fixed(au_category_clean$category, "Type,", 2)

au_category_clean <- separate(data = au_category_clean, col = category, into = c("category", "text"), sep = "Type,")

au_category_clean <- separate(data = au_category_clean, col = text, into = c("text", "remainder"), sep = ",")

au_category_clean

#remove redundant cols
au_category_clean <- mutate(au_category_clean, category=NULL, remainder=NULL)

au_category_clean$category <- au_category_clean$text

au_category_clean <- mutate(au_category_clean, text=NULL)

au_category_clean

#reorder
au_category_clean$id <- as.numeric(au_category_clean$id)

au_category_clean[order(au_category_clean$id, decreasing = FALSE),] 

authors_list
authors_list$id <- as.numeric(authors_list$id)




```




```{r}
#probably need later?
authors_corpus <- VCorpus(VectorSource(authors))
# Extra whitespace is eliminated by:
authors_corpus <- tm_map(authors_corpus, stripWhitespace)
# Conversion to lower case by:
authors_corpus <- tm_map(authors_corpus, content_transformer(tolower))
#Removal of stopwords by:
authors_corpus <- tm_map(authors_corpus, removeWords, stopwords("english"))
#Stemming is done by:
tm_map(authors_corpus, stemDocument)

 dtm <- DocumentTermMatrix(authors_corpus)
 inspect(dtm)



```





```{r}
chart_df <- data.frame(titles, authors, tags, SDGs, weblink)

knitr::kable(chart_df  %>% head(10))

chart_df <- tibble::rowid_to_column(chart_df, "id")

```



```{r}

#unsdglearn_subpage <- read_html("https://www.unsdglearn.org/courses/unido-industrial-analytics-platform-iap/")
```

```{r}
#old code
#keywords_nodes <- html_nodes(unsdglearn_subpage, 
#                           xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", #"keyword", " " ))]')
#keywords <- html_text(keywords_nodes)
#keywords


#audience_nodes <- html_nodes(unsdglearn_subpage, 
#                           xpath = '//*[(@id = "content")]//li')
#audience <- html_text(audience_nodes)
#audience


```
```{r}

# Define the KEYWORDS worker function
scraper <- function(weblink) {
  read_html(weblink) %>% 
    html_nodes(xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "keyword", " " ))]') %>% 
    html_text() %>% 
    enframe("id", "keywords")
}

# Iterate over the urls, applying the function each time
keywords <- map_dfr(weblink, scraper, .id = "id")

```


```{r}

# Define the AUDIENCE worker function
scraper <- function(weblink) {
  read_html(weblink) %>% 
    html_nodes(xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "resource-contentblock", " " )) and (((count(preceding-sibling::*) + 1) = 3) and parent::*)]//*[contains(concat( " ", @class, " " ), concat( " ", "wrapper", " " ))]') %>% 
    html_text() %>% 
    enframe("id", "audience")
}

# Iterate over the urls, applying the function each time
audience <- map_dfr(weblink, scraper, .id = "id")
audience


```


```{r}

#Combine into single columns

keywords_combined <- keywords %>%
 group_by(id) %>%
 summarize(keywords = str_c(keywords, collapse = ", "))

#df3 <- merge(audience, keywords_combined, by = "id")

chart_df <- merge(chart_df, keywords_combined, by = "id")
combined <- merge(chart_df, audience, by = "id", all = TRUE)

write.csv(combined,"C:/Users/caitl/Documents/GitHub/thesis/exports/unsdglearn.csv", row.names = TRUE)


```

```{r}

authors_list <- as.data.frame(authors_list)

write.csv(authors_list,"C:/Users/caitl/Documents/GitHub/mpp_thesis/exports/authors_list.csv", row.names = TRUE)

au_category_clean

au_category_list <- as.data.frame(au_category_clean)

write.csv(au_category_list,"C:/Users/caitl/Documents/GitHub/mpp_thesis/exports/au_category_list.csv", row.names = TRUE)


write.csv(results_urls,"C:/Users/caitl/Documents/GitHub/mpp_thesis/exports/results_urls.csv", row.names = TRUE)

write.csv(au_category_clean,"C:/Users/caitl/Documents/GitHub/mpp_thesis/exports/au_category_clean.csv", row.names = TRUE)


write.csv(authors_list_A,"C:/Users/caitl/Documents/GitHub/mpp_thesis/exports/authors_list_A.csv", row.names = TRUE)

```

```{r}
#long way
#------------------------UN PROGRAMS

un_programs <- c('UN-Habitat|UNDP|UNEP|UN Global Compact')

df <- data.frame(unique(authors_list_A$authors), un_programs = 0)
colnames(df)[1] <- "authors"
df[grep(un_programs, df$authors, value = F), "un_programs"] <- 1
df <- df[order(df$un_programs, decreasing = TRUE),] %>% unique()

df

#------------------------UN SPECIALIZED AGENCIES
un_specialized_agencies <-
  c('FAO|Food and Agriculture Organization of the United Nations|ICAO|International Civil Aviation Organization|IFAD|International Fund for Agricultural Development|ILO|International Labour Organization|IMF|International Monetary Fund|IMO|International Maritime Organization|ITU|International Telecommunication Union|UNESCO|United Nations Educational Scientific and Cultural Organization|UNIDO|United Nations Industrial Development Organization|UNWTO|World Tourism Organization|UPU|Universal Postal Union|WHO|World Health Organization|WIPO|World Intellectual Property Organization|WMO|World Meteorological Organization|World Bank Group|World Bank')
df_agent <- data.frame(authors_list_A$authors, un_specialized_agencies = 0)
colnames(df_agent)[1] <- "authors"
df_agent[grep(pattern = un_specialized_agencies, df_agent$authors, value = F), "un_specialized_agencies"] <- 1
df_agent <- df_agent[order(df_agent$un_specialized_agencies, decreasing = TRUE),] %>% unique()

df_agent

#------------------------UN
un <-
  c('UN DESA|UN Statistics Division|United Nations Statistics Division|United Nations Global Compact Academy|UNDCO|UNGCA|UNITAR')
df_un <- data.frame(authors_list_A$authors, un = 0)
colnames(df_un)[1] <- "authors"
df_un[grep(un, df_un$authors, value = F), "un"] <- 1
df_un <- df_un[order(df_un$un, decreasing = TRUE),] %>% unique()
df_un

#sapply(un, function(x) df_un$authors[grepl(x, df_un$authors)])

#sapply(un, function(x) df_un[grepl(x, un)])



#------------------------NONProfit
#space clear rm(unsdglearn,unsdglearn_silos,coursera)
ngo <-
  c('501','nonprofit','non-profit','non-governmental','nongovernmental','Non-governmental organization','Non-governmental','NGO','ngo','ingo','nongovernmental','philanthropic initiative')
df_ngo <- data.frame(authors_list_A$category, ngo = 0)
colnames(df_ngo)[1] <- "category"
df_ngo[grep(pattern = ngo, df_ngo$category, value = F), "ngo"] <- 1
df_ngo <- df_ngo[order(df_ngo$ngo, decreasing = TRUE),] %>% unique()
df_ngo

#sapply(ngo, function(x) df_ngo$authors[grepl(x, df_ngo$authors)])


#------------------



#------------------ If Coded, Change Category
#df
#df_agent 262
#authors_list_A
#add on new columns
#522
#authors_list_A
#df

```

```{r}
#toolbox
#soft_code$id <- seq.int(nrow(soft_code))

```
