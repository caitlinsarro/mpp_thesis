---
title: "test"
author: "Caitlin Sarro"
date: "3/30/2022"
output: html_document
---

```{r setup, include=FALSE}
#install.packages("groupdata2")
library(tidyverse)
library(tidymodels)
library(tm) #for stopwords
library(tidytext)
library(textrecipes)
library(discrim)
library(naivebayes)

#core
library(quanteda.textmodels)

#use to downsample
library(themis)
library(C50)


#picking folds
library(cvms)  # version >= 1.2.2 
library(groupdata2)  # version >= 1.4.1
library(dplyr)
library(ggplot2)
```
```{r}
packages = c('plotROC')

for(p in packages){
  if(!require(p, character.only = T)){
    install.packages(p)
  }
  library(p, character.only = T)
}

```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r}
all_courses_coded <- read_csv("data/final_coded_dataset.csv")
#all_courses_coded

#all_courses_coded <- na.omit(all_courses_coded)
```





```{r}
#need to add in (at least) three new subsets before doing the model test

#TEST of this https://cfss.uchicago.edu/notes/supervised-text-classification/
#soft_coded_long <- read_csv("data/soft_coded_long2.csv")
#soft_coded_long <- read_csv("data/soft_hard_codes_combined.csv")
theme_set(theme_minimal())
set.seed(444)

#all_courses_coded <- read_csv("exports/all_courses_coded.csv")

#all_courses_coded <- read_csv("data/soft_hard_codes_combined.csv")

all_courses_coded <- all_courses_coded %>% select(id, titles,category,description, major_categ)

all_courses_coded$major_categ <- as.factor(all_courses_coded$major_categ)


all_courses_coded
```



```{r}
#split the dataset 

all_courses_coded_split <- initial_split(data = all_courses_coded, strata = major_categ, prop = .8)

courses_train <- training(all_courses_coded_split)

courses_test <- testing(all_courses_coded_split)
```

```{r}
#preprocessing the data frame


courses_rec <- recipe(category ~ description, data = courses_train)

#congress_rec <- recipe(major ~ text, data = congress_train)
```

```{r}

#final redundant processing the text

courses_rec <- courses_rec %>%
  step_tokenize(description) %>%
  step_stopwords(description) %>%
  step_tokenfilter(description) %>%
  step_tfidf(description)


```

```{r}
#train a model
nb_spec <- naive_Bayes() %>%
  set_mode("classification") %>%
  set_engine("naivebayes")

nb_spec
```

```{r}
## Naive Bayes Model Specification (classification)
## 
## Computational engine: naivebayes
nb_wf_s <- workflow() %>%
  add_recipe(courses_rec) %>%
  add_model(nb_spec)
nb_wf_s

```


```{r}
#train
nb_wf_s %>%
  fit(data = courses_train)
```


```{r}
# Generate sequence of `k` settings in the 3-50 range
fold_counts <- round(seq(from = 3, to = 50, length.out = 10))
# Repeat each 3 times
fold_counts <- rep(fold_counts, each = 3)

fold_counts

```


```{r}
#evaluate
set.seed(444)


courses_folds <- vfold_cv(data = courses_train, strata = major_categ, v = 20, repeats = 2)
courses_folds

#soft_code_folds <- vfold_cv(data = courses_train, strata = major_categ, v = 10)

```

```{r}
nb_cv_s <- nb_wf_s %>%
  fit_resamples(
    courses_folds,
    control = control_resamples(save_pred = TRUE)
  )

```

```{r}
#We can extract relevant information using collect_metrics() and collect_predictions().

nb_cvs_metrics <- collect_metrics(nb_cv_s)
nb_cvs_predictions <- collect_predictions(nb_cv_s)

nb_cvs_metrics
nb_cvs_predictions

```

```{r}
#receiver operator curve plot that shows the sensitivity at different thresholds. It demonstrates how well a classification model can distinguish between classes.

nb_cvs_predictions %>%
  group_by(id) %>%
  roc_curve(truth = category, c(starts_with(".pred"), -.pred_class)) %>%
  autoplot() +
  labs(
    color = NULL,
    title = "Sustainable Governance Aspects Curve for SDG Trainings",
    subtitle = "Each resample fold is shown in a different color")  + theme_minimal()
```

```{r}
conf_mat_resampled(x = nb_cv_s, tidy = FALSE) %>%
  autoplot(type = "heatmap") +
  scale_y_discrete(labels = function(x) str_wrap(x, 20)) +
  scale_x_discrete(labels = function(x) str_wrap(x, 20))

```


```{r}
#Compare to the null model
null_classification <- null_model() %>%
  set_engine("parsnip") %>%
  set_mode("classification")

null_cv <- workflow() %>%
  add_recipe(courses_rec) %>%
  add_model(null_classification) %>%
  fit_resamples(
    courses_folds
  )

null_cv %>%
  collect_metrics()

#Notice the accuracy is the same as for the naive Bayes model. This is because naive Bayes still leads to every observation predicted as “Health”, which is the exact same result as the null model. Clearly we need a better modeling strategy.

```


```{r}
all_courses_coded$major_categ <- as.factor(all_courses_coded$major_categ)
ggplot(data = all_courses_coded, mapping = aes(x = fct_infreq(major_categ) %>% fct_rev())) +
  geom_bar() +
  coord_flip() +
  labs(
    title = "Distribution of Governance Category",
    subtitle = "By major policy topic",
    x = NULL,
    y = "Number of training courses"
  )

```

```{r}
##     step_downsample, step_upsample
# build on existing recipe
courses_rec <- courses_rec %>%
  step_downsample(major_categ)
courses_rec
```















```{r}
#alternative modeling approach
tree_spec <- decision_tree() %>%
  set_mode("classification") %>%
  set_engine("C5.0")

tree_spec

tree_wf <- workflow() %>%
  add_recipe(courses_rec) %>%
  add_model(tree_spec)

tree_wf

```
```{r}
set.seed(100)

tree_cv <- fit_resamples(
  tree_wf,
  courses_folds,
  control = control_resamples(save_pred = TRUE)
)
tree_cv

collect_notes(tree_cv)
```



```{r}
# Split Data into Training and Testing in R 
sample_size = floor(0.8*nrow(all_courses_coded))
set.seed(444)

# randomly split data in r
picked <- sample(seq_len(nrow(all_courses_coded)),size = sample_size)
train_picked <- all_courses_coded[picked,]
#training set
dfmat_train <- dfm(tokens(train_picked$description, remove_punct = TRUE))


#unlabeled test set
dfmat_test <- all_courses_coded[-picked,]
dfmat_test <- dfmat_test %>% select(-c(category,))
dfmat_test <- dfmat_test %>% select(-c(major_categ,))
#testing set
dfmat_test <- dfm(tokens(dfmat_test$description, remove_punct = TRUE))

#set category levels

class <- c("participation","policy coherence","democratic institutions","reflexivity and adaptation")

#train model 
tmod_nb <- textmodel_nb(dfmat_train, class)

#predict class
tmod_nb_predict <- predict(tmod_nb, dfmat_test, force = TRUE)

#convert(dfmat_train, to= "data.frame")



```

```{r}

#create a cross-table

train_cat_table <- train_picked$category
tmod_nb_predict

train_cat_table <- table(train_cat_table)
train_cat_table <- as.data.frame(train_cat_table)

tmod_nb_predict_table <- table(tmod_nb_predict)
train_cat_table
tmod_nb_predict_table <- as.data.frame(tmod_nb_predict_table)

colnames(train_cat_table)[1] <- "category"
colnames(tmod_nb_predict_table)[1] <- "category"

tab <- table(actual = train_cat_table,
             predicted = tmod_nb_predict_table)

print(tab)
```



```{r}
#create a cross-table

train_cat_table <- train_picked$category

train_cat_table <- table(train_cat_table)
train_cat_table <- as.data.frame(train_cat_table)

tmod_nb_predict_table <- table(tmod_nb_predict)
train_cat_table
tmod_nb_predict_table <- as.data.frame(tmod_nb_predict_table)

colnames(train_cat_table)[1] <- "category"
colnames(tmod_nb_predict_table)[1] <- "category"

train_cat_table$model <- "actual"
tmod_nb_predict_table$model <- "predicted"

crosstab <- rbind(train_cat_table,tmod_nb_predict_table)


tab <- table(actual = crosstab$category[crosstab$model=="actual"],
             predicted = crosstab$category[crosstab$model=="predicted"])

print(tab)

```
