---
title: "Web Scraping"
subtitle: "UNSDGLEARN: Capacity Building"
author: "Caitlin Sarro"
date: "2/9/2022"
output: 
  html_document:
    toc: TRUE
    df_print: paged
    number_sections: FALSE
    highlight: tango
    theme: lumen
    toc_depth: 3
    toc_float: true
    css: custom.css 
    self_contained: false
    includes:
      after_body: footer.html
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```
***
CONTEXT TEXT

In general, remember, the basic **workflow for scraping static webpages** is the following.

**Step 1.** Load the packages

```{r, message=F}
#install.packages("")
library(RSelenium)
library(tidyverse)
library(rvest)
library(V8)


library(stringr)
library(readr)
library(shiny)

library(reshape2)
library(dplyr)

library(purrr)
library(tibble)

```



```{r}
#https://unpan.un.org/capacity-development/curriculum-on-governance-for-the-SDGs


```

# Scraping multiple pages `r emo::ji("robot")`

Whenever you want to really understand what's going on within the functions of a new R package, it is very likely that there is a relevant article published in the [Journal of Statistical Software](https://www.jstatsoft.org/index). Let's say you are interested in how the journal was doing over the past years.

**Step 1.** Inspect the source. Basically, follow steps to extract the Xpath information.
```{r, eval=F}
#browseURL("https://www.unsdglearn.org/courses/?_sf_s=capacity%20building")
```

**Step 2.** Parse the page source.

```{r}

unpan_web <- "https://unpan.un.org/capacity-development/curriculum-on-governance-for-the-SDGs"
unpan <- read_html(unpan_web)


```

```{r}
#UNPAN
#start RSelenium
remDr <- remoteDriver(
        remoteServerAdd = "localhost",
        port = 4445L,
        browser = "chrome"
)

remDr$open()

#navigate to your page
remDr$navigate("https://unpan.un.org/capacity-development/curriculum-on-governance-for-the-SDGs")

#scroll down 5 times, waiting for the page to load at each time
for(i in 1:5){      
remDr$executeScript(paste("scroll(0,",i*10000,");"))
Sys.sleep(3)    
}

#get the page html
page_source <-remDr$getPageSource()

#pageEls  <- remDr$findElements(using = "css", "#contents #details #meta")

unpan <- read_html(page_source[[1]])

unpan
#parse it
#html(page_source[[1]]) %>% html_nodes(".product-itm-price-new") %>%
#  html_text()



#unsdglearn <- read_html("https://www.unsdglearn.org/courses/?sf_data=results&_sf_s=capacity%20building&sf_paged=2")

#unsdglearn
```
**Step 3.** Extract information.

```{r}
body_nodes <- unpan %>% 
 html_node("body") %>% 
 html_children()

body_nodes %>% 
 html_children()


parsed_nodes <- html_nodes(unpan, 
                           xpath = '//*//*[contains(concat( " ", @class, " " ), concat( " ", "training_title", " " ))]')
titles_unpan <- html_text(parsed_nodes)
titles_unpan

weblink <- html_nodes(unpan, 
                           xpath = '//a') %>% html_attr("href")


#remove repeat/ mistake row
#titles_unpan <- as.data.frame(titles_unpan)
#titles_unpan <- titles_unpan[-c(1,2),]
#titles_unpan
#remove blank row

weblink
#remove repeat/ mistake row
weblink <- as.data.frame(weblink)
weblink <- weblink %>% slice(47:57)
weblink 



parsed_nodes <- html_nodes(unpan, 
                           xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "training_content", " " ))]')
authors_unpan <- html_text(parsed_nodes)
authors_unpan

#cleanup
authors_unpan <- gsub("\nTraining of Trainers | EnglishAccess the Toolkit\n ", "", authors_unpan)
authors_unpan <- gsub("| EnglishAccess the Toolkit\n ", "", authors_unpan)
authors_unpan


```


```{r}
parsed_nodes <- html_nodes(unpan, 
                           xpath = '//*[(@id = "block-custom-content")]//*[contains(concat( " ", @class, " " ), concat( " ", "field__item", " " ))]//div//div//div//img') %>%
  html_attr("alt" )%>%
    enframe("id", "SDGs")
sdgs_unpan <- html_text(parsed_nodes)
sdgs_unpan


html_nodes(xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "goals", " " ))] | //*[contains(concat( " ", @class, " " ), concat( " ", "goal-image", " " ))]//img') %>%
  html_attr("alt") %>%
    enframe("id", "SDGs")


#add SDGs by hand
A <- c(16)
B <- c(16)
C <- c(14,16,17)
D <- c(11,16,17)
E <- c(11,16,17)
F <- c(10,16)
G <- c(9,11,13,14,15,16)
H <- c(1,2,3,4,5,6,7,8,9,11,15,16,17)
I <- c(16,17)
J <- c(1,2,13,14,16,17)
K <- c(NA)

```
```{r}
#remove 
#https://egov4women.unescapsdd.org/toolkit
#https://www.unitar.org/event/full-catalog/toolkit-integrated-policies-and-policy-coherence-sdgs
#https://www.unitar.org/event/full-catalog/integrated-recovery-planning-and-policy-coherence-towards-sdgs\n

weblink_node <- weblink %>% slice(1:8)

weblink_node$weblink <- paste0("https://unpan.un.org", weblink_node$weblink)
weblink_node <- as.list(weblink_node)

weblink_node <- (weblink_node$weblink)



```


```{r}
#Parse out content from the courses

scraper <- function(weblink_node) {read_html(weblink_node) %>% 
    html_nodes(xpath = '//p') %>% 
    html_text() %>% 
    enframe("id", "description")
}

# Iterate over the urls, applying the function each time
keywords_unpan <- map_dfr(weblink_node, scraper, .id = "id")


```

```{r}
#Combine into single columns

descrp_unpan <- keywords_unpan %>%
 group_by(id) %>%
 summarize(description = str_c(description, collapse = ", "))


descrp_unpan$id <- as.numeric(descrp_unpan$id)

descrp_unpan[order(descrp_unpan$id, decreasing = FALSE),] 

descrp_unpan

```

```{r}
#quick pull from the last three
#1) https://egov4women.unescapsdd.org/toolkit -- which is actually https://egov4women.unescapsdd.org/toolkit/introduction
#2) https://www.unitar.org/event/full-catalog/toolkit-integrated-policies-and-policy-coherence-sdgs which is https://www.unitar.org/event/print/full-catalog/toolkit-integrated-policies-and-policy-coherence-sdgs
#3) https://www.unitar.org/event/full-catalog/integrated-recovery-planning-and-policy-coherence-towards-sdgs\n which is actually https://www.unitar.org/event/print/full-catalog/integrated-recovery-planning-and-policy-coherence-towards-sdgs


AL <- read_html("https://egov4women.unescapsdd.org/toolkit/introduction") %>% 
    html_nodes(xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "field-body", " " ))] | //p[(((count(preceding-sibling::*) + 1) = 1) and parent::*)]') %>% 
    html_text() %>% 
    enframe("id", "description") %>% 
 summarize(description = str_c(description, collapse = ", "))

AL
AL$id <- seq.int(nrow(AL))

### second one
BL <- read_html("https://www.unitar.org/event/print/full-catalog/toolkit-integrated-policies-and-policy-coherence-sdgs") %>% 
    html_nodes(xpath = '//li | //p | //*[contains(concat( " ", @class, " " ), concat( " ", "table-row", " " ))]//*[contains(concat( " ", @class, " " ), concat( " ", "field-value", " " ))] | //h2') %>% 
    html_text() %>% 
    enframe("id", "description") %>% 
 summarize(description = str_c(description, collapse = ", "))

BL
BL$id <- seq.int(nrow(BL))

### third one

CL <- read_html("https://www.unitar.org/event/print/full-catalog/integrated-recovery-planning-and-policy-coherence-towards-sdgs") %>% 
    html_nodes(xpath = '//li | //p | //*[contains(concat( " ", @class, " " ), concat( " ", "table-row", " " ))]//*[contains(concat( " ", @class, " " ), concat( " ", "field-value", " " ))] | //h2') %>% 
    html_text() %>% 
    enframe("id", "description") %>% 
 summarize(description = str_c(description, collapse = ", "))

CL
CL$id <- seq.int(nrow(CL))

descrp_unpan <- rbind(descrp_unpan, AL, BL, CL)
descrp_unpan$id <- seq.int(nrow(descrp_unpan))
descrp_unpan

```



```{r}
titles_unpan
weblink



chart_df <- data.frame(titles_unpan, authors_unpan, weblink, descrp_unpan)

#clean up authors
unpan_df <- chart_df %>% mutate(authors_unpan = "un")


#unpan_df <- merge(chart_df, descrp_unpan, by="id")


#chart_df <- tibble::rowid_to_column(chart_df, "id")

#titles	authors	tags	SDGs	weblink	keywords	audience


```

```{r}
#export as CSV
write.csv(unpan_df,"C:/Users/caitl/Documents/GitHub/thesis/exports/unpan_df.csv", row.names = TRUE)

```

```{r}
#not working?
#system("sudo docker pull selenium/standalone-chrome",wait=T)
#Sys.sleep(5)
#system("sudo docker run -d -p 4445:4444 selenium/standalone-chrome",wait=T)
#Sys.sleep(5)
#remDr <- remoteDriver(port=4445L, browserName="chrome")
#Sys.sleep(15)
#remDr$open()
#docker run -d -p 4445:4444 selenium/standalone-chrome

#tend to keep using
#df_cat$id <- seq.int(nrow(df_cat))

```