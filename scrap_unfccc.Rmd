---
title: "Web Scraping"
subtitle: "UNSDGLEARN: Capacity Building"
author: "Caitlin Sarro"
date: "2/9/2022"
output: 
  html_document:
    toc: TRUE
    df_print: paged
    number_sections: FALSE
    highlight: tango
    theme: lumen
    toc_depth: 3
    toc_float: true
    css: custom.css 
    self_contained: false
    includes:
      after_body: footer.html
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```
***
CONTEXT TEXT

In general, remember, the basic **workflow for scraping static webpages** is the following.

**Step 1.** Load the packages

```{r, message=F}
#install.packages("")
library(RSelenium)
library(tidyverse)
library(rvest)
library(V8)


library(stringr)
library(readr)
library(shiny)

library(reshape2)
library(dplyr)

library(purrr)
library(tibble)

```



```{r}
#https://unfccc.int/topics/capacity-building/workstreams/capacity-building-portal/capacity-building-e-learning#eq-1


```

# Scraping multiple pages `r emo::ji("robot")`

Whenever you want to really understand what's going on within the functions of a new R package, it is very likely that there is a relevant article published in the [Journal of Statistical Software](https://www.jstatsoft.org/index). Let's say you are interested in how the journal was doing over the past years.

**Step 1.** Inspect the source. Basically, follow steps to extract the Xpath information.
```{r, eval=F}
#browseURL("https://www.unsdglearn.org/courses/?_sf_s=capacity%20building")
```

**Step 2.** Parse the page source.

```{r}

unfcccpage <- "https://unfccc.int/topics/capacity-building/workstreams/capacity-building-portal/capacity-building-e-learning#eq-1"
unfccc <- read_html(unfcccpage)


```

```{r}
#UNFCCC 
#start RSelenium
remDr <- remoteDriver(
        remoteServerAdd = "localhost",
        port = 4445L,
        browser = "chrome"
)

remDr$open()

#navigate to your page
remDr$navigate("https://unfccc.int/topics/capacity-building/workstreams/capacity-building-portal/capacity-building-e-learning#eq-1")

#scroll down 5 times, waiting for the page to load at each time
for(i in 1:5){      
remDr$executeScript(paste("scroll(0,",i*10000,");"))
Sys.sleep(3)    
}

#get the page html
page_source <-remDr$getPageSource()

#pageEls  <- remDr$findElements(using = "css", "#contents #details #meta")

unfccc <- read_html(page_source[[1]])

unfccc
#parse it
#html(page_source[[1]]) %>% html_nodes(".product-itm-price-new") %>%
#  html_text()



#unsdglearn <- read_html("https://www.unsdglearn.org/courses/?sf_data=results&_sf_s=capacity%20building&sf_paged=2")

#unsdglearn
```
**Step 3.** Extract information.

```{r}
body_nodes <- unfccc %>% 
 html_node("body") %>% 
 html_children()

body_nodes %>% 
 html_children()


parsed_nodes <- html_nodes(unfccc, 
                           xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "text-align-left", " " )) and (((count(preceding-sibling::*) + 1) = 1) and parent::*)]')
titles_unfccc <- html_text(parsed_nodes)
titles_unfccc


titles_unfccc <- as.data.frame(titles_unfccc)
titles_unfccc[titles_unfccc == "Climate Responsive Budgeting(link is external)Short info: This tutorial introduces how governments can respond to the climate change challenge through better budgeting, which integrates climate change risks and opportunities into budget preparation."] <- "Climate Responsive Budgeting"


titles_unfccc <- as.data.frame(titles_unfccc)
titles_unfccc[titles_unfccc == "\nREDD+ Learning Session 69: #NDCsWeWant - NDC Policy Recommendations on Forests and Food\n\n(link is external) \n\n\nShort info: In this learning session, presenters will provide decision-makers with joint policy recommendations to maximize forest and food related NBS as they revise their NDCs to meet the ambition of the Paris Agreement. Amanda McKee from the NDC Partnership will introduce the webinar and set the scene, Fernanda de Carvalho from WWF will present the organization’s checklist for assessing the #NDCsWeWant, and Franziska Haupt and Haseeb Bakhtary from Climate Focus will discuss key findings from “Enhancing NDCs for Food Systems” and a forthcoming report “Enhancing forest targets and measures in NDCs.”Providing Institution:  WWF Forest and ClimateRegion: GlobalType of Activities: Online Course(link is external)Learn more\n"] <- " "

titles_unfccc




#remove repeat/ mistake row
titles_unfccc <- as.data.frame(titles_unfccc)
titles_unfccc2 <- titles_unfccc[-c(40),]

#remove blank row


```


```{r}
#pull in text from website as large chunk
parsed_nodes <- html_nodes(unfccc, 
                           xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "table-container", " " )) ]')
text_unfccc <- html_text(parsed_nodes)
text_unfccc
```

```{r}
#set up dataframe and standardize capitazlisation (to lowercase)

unfccc_df_mess <- text_unfccc %>% enframe("id", "text")

unfccc_df_mess$text = tolower(unfccc_df_mess$text)

#separate title text from body
unfccc_df <- str_split_fixed(unfccc_df_mess$text, "short info:", 2)

unfccc_df <-as.data.frame(unfccc_df)

unfccc_df <- data.frame(do.call('rbind', strsplit(as.character(unfccc_df$V2),'providing institution',fixed=TRUE)))

unfccc_df <- separate(data = unfccc_df_mess, col = text, into = c("left", "keywords"), sep = "short info:")

unfccc_df <- separate(data = unfccc_df, col = keywords, into = c("keywords", "author"), sep = "providing institution:")

unfccc_df <- separate(data = unfccc_df, col = author, into = c("author", "region"), sep = "region:")

unfccc_df <- separate(data = unfccc_df, col = region, into = c("region", "dump"), sep = "type of activities:")

#remove redundant cols
unfccc_df <- mutate(unfccc_df, id=NULL, left=NULL, dump=NULL)


```

```{r}
#extract the links

body_nodes <- unfccc %>% 
 html_node("body") %>% 
 html_children()

body_nodes %>% 
 html_children()

weblink <- unfccc  %>% html_nodes("div.embedded-entity-content > a") %>% html_attr("href")

weblink

```



```{r}
weblink

weblinks2 <- weblink %>%  enframe("id", "weblink")

#subset for coursera
coursera_subset <- weblinks2 %>%
  filter(str_detect(weblink, "coursera"))

#subset for elearning
elearning_subset <- weblinks2 %>%
  filter(str_detect(weblink, "elearning"))

#subset for unccelearn
unccelearn_subset <- weblinks2 %>%
  filter(str_detect(weblink, "unccelearn.org"))

#subset for worldbank
worldbank_subset <- weblinks2 %>%
  filter(str_detect(weblink, "olc.worldbank.org"))


elearning_subset
#fix mistake in elearning

elearning_subset <- elearning_subset[-c(4), ] 

unccelearn_subset
worldbank_subset

worldbank_subset[nrow(worldbank_subset) + 1,] = c(56,"https://olc.worldbank.org/content/elearning-course-passive-urban-cooling-solutions")
worldbank_subset
```

```{r}
#Pull out longer descriptions ELEARN
weblink <- elearning_subset$weblink

# Define the DESCRIPTION worker function
scraper <- function(weblink) {
  read_html(weblink) %>% 
    html_nodes(xpath = '//h1 | //*[contains(concat( " ", @class, " " ), concat( " ", "thematic-area", " " ))]//span | //*[contains(concat( " ", @class, " " ), concat( " ", "content", " " ))]//p') %>% 
    html_text() %>% 
    enframe("id", "description") %>%
    mutate(weblink = weblink) 
}

# Iterate over the urls, applying the function each time
descrp_elearning <- map_dfr(weblink, scraper, .id = "id")
descrp_elearning




```

```{r}
#Pull out longer descriptions ELEARN
weblink <- unccelearn_subset$weblink

# Define the DESCRIPTION worker function
scraper <- function(weblink) {
  read_html(weblink) %>% 
    html_nodes(xpath = '//h1 | //*[(@id = "section-5")]//*[contains(concat( " ", @class, " " ), concat( " ", "no-overflow", " " ))] | //*[(@id = "section-4")]//li | //*[(@id = "section-3")]//p | //*[(@id = "section-0")]//p') %>% 
    html_text() %>% 
    enframe("id", "description") %>%
    mutate(weblink = weblink) 
}

# Iterate over the urls, applying the function each time
descrp_unccelearn <- map_dfr(weblink, scraper, .id = "id")
descrp_unccelearn



```
```{r}
#Pull out longer descriptions WORLDBANK
weblink <- worldbank_subset$weblink

# Define the DESCRIPTION worker function
scraper <- function(weblink) {
  read_html(weblink) %>% 
    html_nodes(xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "field-type-taxonomy-term-reference", " " ))] | //*[contains(concat( " ", @class, " " ), concat( " ", "series-content", " " ))]//h2 | //*[contains(concat( " ", @class, " " ), concat( " ", "field-type-text-with-summary", " " ))]//*[contains(concat( " ", @class, " " ), concat( " ", "even", " " ))]') %>% 
    html_text() %>% 
    enframe("id", "description") %>%
    mutate(weblink = weblink) 
}

# Iterate over the urls, applying the function each time
descrp_worldbank <- map_dfr(weblink, scraper, .id = "id")
descrp_worldbank




```

```{r}
titles_unfccc
weblink
unfccc_df
unfccc_df$id <- seq.int(nrow(unfccc_df))
titles_unfccc$id <- seq.int(nrow(titles_unfccc))
weblink$id <- seq.int(nrow(weblink))

chart_df <- merge(titles_unfccc, unfccc_df, by="id")
chart_df <- left_join(chart_df, weblink, by="id")

weblink <- data.frame(weblink)

knitr::kable(chart_df  %>% head(10))

weblink <- tibble::rowid_to_column(weblink, "id")

#titles	authors	tags	SDGs	weblink	keywords	audience


```

```{r}
#export as CSV
write.csv(coursera_subset,"C:/Users/caitl/Documents/GitHub/thesis/exports/coursera.csv", row.names = TRUE)

```

```{r}
#not working?
#system("sudo docker pull selenium/standalone-chrome",wait=T)
#Sys.sleep(5)
#system("sudo docker run -d -p 4445:4444 selenium/standalone-chrome",wait=T)
#Sys.sleep(5)
#remDr <- remoteDriver(port=4445L, browserName="chrome")
#Sys.sleep(15)
#remDr$open()

```