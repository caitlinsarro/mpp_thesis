---
title: "Web Scraping"
subtitle: "Author Standardization"
author: "Caitlin Sarro"
date: "2/9/2022"
output: 
  html_document:
    toc: TRUE
    df_print: paged
    number_sections: FALSE
    highlight: tango
    theme: lumen
    toc_depth: 3
    toc_float: true
    css: custom.css 
    self_contained: false
    includes:
      after_body: footer.html
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```
***


**Step 1.** Load the packages `rvest` and `stringr`.

```{r, message=F}
#install.packages("")
library(RSelenium)
library(tidyverse)
library(rvest)
library(stringr)
library(readr)

library(reshape2)
library(dplyr)

library(xml2)
library(purrr)
library(tibble)

library(tm)

library(RCurl)

```



```{r}
#Capacity Building https://www.unsdglearn.org/courses/?_sf_s=capacity%20building
#Breaking the Silos https://www.unsdglearn.org/courses/?_sfm_sdg=5



coursera <- read_csv("exports/coursera_c.csv")
unsdglearn_silos <- read_csv("exports/unsdglearn_silos.csv")
unsdglearn <- read_csv("exports/unsdglearn.csv")
sdgacademy <- read_csv("exports/sdgacademy.csv")


authors_cour <- coursera %>% select(titles, author)
authors_unsdglearn_silos <- unsdglearn_silos %>% select(titles, authors) #fix this s
authors_unsdglearn <- unsdglearn %>% select(titles, authors) #fix this s
authors_sdgacademy <- sdgacademy %>% select(titles_acad, author) #fix this standardize

#fix nonstandard names to titles and authors
authors_sdgacademy <- authors_sdgacademy %>%
  rename(titles = titles_acad)

authors_sdgacademy <- authors_sdgacademy %>%
  rename(authors = author)

authors_cour <- authors_cour %>%
  rename(authors = author)


#combine into one set
combined <- rbind(authors_cour, authors_unsdglearn_silos, authors_unsdglearn, authors_sdgacademy)

```





```{r}
#to long data
combined <- combined %>% 
  mutate(authors = strsplit(as.character(authors), ",")) %>%
  unnest(authors)

#trim leading and ending whitespaces
combined$authors <- trimws(combined$authors)

authors_list <- combined


#make a dataframe
#duplicate column
authors_list$au_category <- authors_list$authors

```


```{r}
#this category to transform
authors_list$au_category

#making the website link for a google search
#replace whitespace with + 
authors_list$piece <- gsub(" ", "_", authors_list$authors)

authors_list$weblink <- paste0("https://wikipedia.org/wiki/", authors_list$piece)
#authors_list$weblink <- paste0("https://www.google.com/search?q=", authors_list$piece)

weblink_au <- authors_list$weblink

unique(weblink_au)
url.exists("weblink_au")


authors_list

```
```{r}
#
#start RSelenium

system('docker kill $(docker ps -q)')
Sys.sleep(2)
system('docker run -d -p 4445:4444 selenium/standalone-chrome-debug:latest')
Sys.sleep(2)
system('docker container ls')

#rstudioapi::terminalExecute('java -Dwebdriver.chrome.driver="C:/Users/caitl/Documents/R #Files/selenium/chromedriver.exe" -jar "C:/Users/caitl/Documents/R #Files/selenium/selenium-server-standalone-3.141.59.jar" -port 4567')
#standalone-chrome-debug:latest


remDr <- remoteDriver(
        remoteServerAdd = "localhost",
        port = 4445L,
        browser = "chrome" 
)

remDr$open()



```


```{r}


# Run a test
weblink_test <-"https://en.wikipedia.org/wiki/Lund_University"

test <- read_html(weblink_test) %>% 
    html_nodes(xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "infobox-data", " " ))] | //*[contains(concat( " ", @class, " " ), concat( " ", "infobox-label", " " ))]') %>% 
    html_text()

test



```




```{r}
# Finding and fixing NOT-REAL-LINKS
url.exists("weblink_au")

scraper <- function(weblink_au) { tryCatch(
  read_html(weblink_au) %>% 
    html_nodes(xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "infobox-data", " " ))] | //*[contains(concat( " ", @class, " " ), concat( " ", "infobox-label", " " ))]') %>% 
    html_text() %>% 
    enframe("id", "category"),
    error = function(err) return(NULL))
}

```


```{r}

url_exists <- function(x, non_2xx_return_value = FALSE, quiet = FALSE,...) {

  suppressPackageStartupMessages({
    require("httr", quietly = FALSE, warn.conflicts = FALSE)
  })

  # you don't need thse two functions if you're alread using `purrr`
  # but `purrr` is a heavyweight compiled pacakge that introduces
  # many other "tidyverse" dependencies and this doesnt.

  capture_error <- function(code, otherwise = NULL, quiet = TRUE) {
    tryCatch(
      list(result = code, error = NULL),
      error = function(e) {
        if (!quiet)
          message("Error: ", e$message)

        list(result = otherwise, error = e)
      },
      interrupt = function(e) {
        stop("Terminated by user", call. = FALSE)
      }
    )
  }

  safely <- function(.f, otherwise = NULL, quiet = TRUE) {
    function(...) capture_error(.f(...), otherwise, quiet)
  }

  sHEAD <- safely(httr::HEAD)
  sGET <- safely(httr::GET)

  # Try HEAD first since it's lightweight
  res <- sHEAD(x, ...)

  if (is.null(res$result) || 
      ((httr::status_code(res$result) %/% 200) != 1)) {

    res <- sGET(x, ...)

    if (is.null(res$result)) return(NA) # or whatever you want to return on "hard" errors

    if (((httr::status_code(res$result) %/% 200) != 1)) {
      if (!quiet) warning(sprintf("Requests for [%s] responded but without an HTTP status code in the 200-299 range", x))
      return(non_2xx_return_value)
    }

    return(TRUE)

  } else {
    return(TRUE)
  }

}

results_urls <- data.frame(
  exists = sapply(weblink_au, url_exists, USE.NAMES = FALSE),
  weblink_au,
  stringsAsFactors = FALSE
) 

#%>% dplyr::tbl_df() %>% print()

results_urls

results_urls_fix <- results_urls[results_urls$exists == 'FALSE',]
results_urls_t <- results_urls[results_urls$exists == 'TRUE',]




```



```{r}

weblink_au_t <- results_urls_t$weblink_au


# Define the AUTHORS CATEGORY worker function
scraper <- function(weblink_au_t) {read_html(weblink_au_t) %>% 
    html_nodes(xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "infobox-data", " " ))] | //*[contains(concat( " ", @class, " " ), concat( " ", "infobox-label", " " ))]') %>% 
    html_text() %>% 
    enframe("id", "category")
}


# Iterate over the urls, applying the function each time
au_category_t <- map_dfr(weblink_au_t, scraper, .id = "id")

au_category_t

#    html_text() %>% 
#    enframe("id", "category"),
#    error = function(err) {print(NA)}
#  )}
```









```{r}

# Define the AUTHORS CATEGORY worker function
scraper <- function(weblink_au) { tryCatch(
  read_html(weblink_au) %>% 
    html_nodes(xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "infobox-data", " " ))] | //*[contains(concat( " ", @class, " " ), concat( " ", "infobox-label", " " ))]') %>% 
    html_text() %>% 
    enframe("id", "category"),
    error = function(err) return(NULL))
}


# Iterate over the urls, applying the function each time
au_category <- map_dfr(weblink_au, scraper, .id = "id")

au_category

#    html_text() %>% 
#    enframe("id", "category"),
#    error = function(err) {print(NA)}
#  )}
```








```{r}
#group and summarize
au_category_clean <- au_category_t %>%
 group_by(id) %>%
 summarize(category = str_c(category, collapse = ", "))
au_category_clean 


#separate type text from category
#au_category_clean <- str_split_fixed(au_category_clean$category, "Type,", 2)

au_category_clean <- separate(data = au_category_clean, col = category, into = c("category", "text"), sep = "Type,")

au_category_clean <- separate(data = au_category_clean, col = text, into = c("text", "remainder"), sep = ",")

au_category_clean

#remove redundant cols
au_category_clean <- mutate(au_category_clean, category=NULL, remainder=NULL)

au_category_clean$category <- au_category_clean$text

au_category_clean <- mutate(au_category_clean, text=NULL)

au_category_clean

#reorder
au_category_clean$id <- as.numeric(au_category_clean$id)

au_category_clean[order(au_category_clean$id, decreasing = FALSE),] 


au_category_clean

#Join two sets

urls_with_cate <-merge(results_urls_t, au_category_clean, by.x = "id", by.y = "id", all.x = TRUE)

urls_with_cate 

#remove redundant id
urls_with_cate <- mutate(urls_with_cate, exists=NULL)

```


```{r}
#merge with titles
authors_list_A <- merge(authors_list, urls_with_cate, by.x = "weblink", by.y = "weblink_au", all.x = TRUE)

authors_list_A <- unique(authors_list_A)

authors_list_A 

#remove redundant cols
authors_list_A <- mutate(authors_list_A, weblink=NULL, piece=NULL, au_category=NULL, id=NULL)
authors_list_A

authors_list_A <- unique(authors_list_A)
authors_list_A


```

```{r}
#Universal fixes
#trim leading and ending whitespaces
authors_list_A$category <- trimws(authors_list_A$category)
#lowercase
authors_list_A$category <- tolower(authors_list_A$category)
#remove punctuation
authors_list_A$category <- removePunctuation(authors_list_A$category)
```


```{r}
#author dictionaries

un_specialized_agencies <-
  c(
    "FAO",
    "Food and Agriculture Organization of the United Nations",
    "ICAO",
    "International Civil Aviation Organization",
    "IFAD",
    "International Fund for Agricultural Development",
    "ILO",
    "International Labour Organization",
    "IMF",
    "International Monetary Fund",
    "IMO",
    "International Maritime Organization",
    "ITU",
    "International Telecommunication Union",
    "UNESCO",
    "United Nations Educational Scientific and Cultural Organization",
    "UNIDO",
    "United Nations Industrial Development Organization",
    "UNWTO",
    "World Tourism Organization",
    "UPU",
    "Universal Postal Union",
    "WHO",
    "World Health Organization",
   "WIPO",
    "World Intellectual Property Organization",
    "WMO",
    "World Meteorological Organization",
    "World Bank Group",
    "World Bank"
  )

un_programs <-c("UN-Habitat", "UNDP", "UNEP", "UN Global Compact")

str_detect(authors_list_A$authors, un_specialized_agencies)
authors_list_A$category[authors_list_A$authors

                        
dictionary <- data.frame(search="un_specialized_agencies")
result <- authors_list_A$authors %>%
  filter(grepl(paste(dictionary$search, collapse="|"), text)) 

authors_list_A$C <- ifelse(grepl(un_specialized_agencies, authors_list_A$authors), "yes", "no")
                        
                        
```







```{r}
#manual fixing
authors_list_A$category[authors_list_A$authors=="Academy of Korean Studies"] <- "research institution"

authors_list_A$category[authors_list_A$authors=="African Center for Cities"] <- "research institution"


authors_list_A$category[authors_list_A$authors=="African Center for Cities"] <- "research institution"

authors_list_A$category[authors_list_A$authors=="African Local Government Academy"] <- "government network"

authors_list_A$category[authors_list_A$authors=="Ain Shams University"] <- "public university"

authors_list_A$category[authors_list_A$authors=="Alliance for Global Water Adaptation"] <- "international ngo"

authors_list_A$category[authors_list_A$authors=="American Museum of Natural History"] <- "cultural institution"

authors_list_A$category[authors_list_A$authors=="Athens University of Economics and Business"] <- "public university"


authors_list_A$category[authors_list_A$authors=="Columbia University"] <- "private research university"

authors_list_A$category[authors_list_A$category=="public"] <- "public university"

authors_list_A$category[authors_list_A$category=="private"] <- "private university"

authors_list_A$category[authors_list_A$authors=="Cornell University"] <- "private research university"


authors_list_A$category[authors_list_A$authors=="Belize National Institute of Culture and History"] <- "cultural institution"

authors_list_A$category[authors_list_A$authors=="Banco Interamericano de Desarrollo"] <- "bank"

authors_list_A$category[authors_list_A$authors=="Federal Government"] <- "government"

authors_list_A

authors_list_A$category <- gsub(".*(501(c)(3)|501|nonprofit|non-profit|Non-governmental organization (NGO)).*", "ngo", authors_list_A$category)
authors_list_A$category <- gsub(".*(nonprofit|non-profit).*", "ngo", authors_list_A$category)
authors_list_A$category <- gsub(".*(Non-governmental organization).*", "ngo", authors_list_A$category)



authors_list_A$category



write.csv(authors_list_A,"C:/Users/caitl/Documents/GitHub/mpp_thesis/exports/authors_list_A.csv", row.names = TRUE)


```


```{r}

au_category_clean <- au_category %>%
 group_by(id) %>%
 summarize(category = str_c(category, collapse = ", "))
au_category_clean 


#separate type text from category
#au_category_clean <- str_split_fixed(au_category_clean$category, "Type,", 2)

au_category_clean <- separate(data = au_category_clean, col = category, into = c("category", "text"), sep = "Type,")

au_category_clean <- separate(data = au_category_clean, col = text, into = c("text", "remainder"), sep = ",")

au_category_clean

#remove redundant cols
au_category_clean <- mutate(au_category_clean, category=NULL, remainder=NULL)

au_category_clean$category <- au_category_clean$text

au_category_clean <- mutate(au_category_clean, text=NULL)

au_category_clean

#reorder
au_category_clean$id <- as.numeric(au_category_clean$id)

au_category_clean[order(au_category_clean$id, decreasing = FALSE),] 

authors_list
authors_list$id <- as.numeric(authors_list$id)




```




```{r}
#probably need later?
authors_corpus <- VCorpus(VectorSource(authors))
# Extra whitespace is eliminated by:
authors_corpus <- tm_map(authors_corpus, stripWhitespace)
# Conversion to lower case by:
authors_corpus <- tm_map(authors_corpus, content_transformer(tolower))
#Removal of stopwords by:
authors_corpus <- tm_map(authors_corpus, removeWords, stopwords("english"))
#Stemming is done by:
tm_map(authors_corpus, stemDocument)

 dtm <- DocumentTermMatrix(authors_corpus)
 inspect(dtm)



```





```{r}
chart_df <- data.frame(titles, authors, tags, SDGs, weblink)

knitr::kable(chart_df  %>% head(10))

chart_df <- tibble::rowid_to_column(chart_df, "id")

```



```{r}

#unsdglearn_subpage <- read_html("https://www.unsdglearn.org/courses/unido-industrial-analytics-platform-iap/")
```

```{r}
#old code
#keywords_nodes <- html_nodes(unsdglearn_subpage, 
#                           xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", #"keyword", " " ))]')
#keywords <- html_text(keywords_nodes)
#keywords


#audience_nodes <- html_nodes(unsdglearn_subpage, 
#                           xpath = '//*[(@id = "content")]//li')
#audience <- html_text(audience_nodes)
#audience


```
```{r}

# Define the KEYWORDS worker function
scraper <- function(weblink) {
  read_html(weblink) %>% 
    html_nodes(xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "keyword", " " ))]') %>% 
    html_text() %>% 
    enframe("id", "keywords")
}

# Iterate over the urls, applying the function each time
keywords <- map_dfr(weblink, scraper, .id = "id")

```


```{r}

# Define the AUDIENCE worker function
scraper <- function(weblink) {
  read_html(weblink) %>% 
    html_nodes(xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "resource-contentblock", " " )) and (((count(preceding-sibling::*) + 1) = 3) and parent::*)]//*[contains(concat( " ", @class, " " ), concat( " ", "wrapper", " " ))]') %>% 
    html_text() %>% 
    enframe("id", "audience")
}

# Iterate over the urls, applying the function each time
audience <- map_dfr(weblink, scraper, .id = "id")
audience


```


```{r}

#Combine into single columns

keywords_combined <- keywords %>%
 group_by(id) %>%
 summarize(keywords = str_c(keywords, collapse = ", "))

#df3 <- merge(audience, keywords_combined, by = "id")

chart_df <- merge(chart_df, keywords_combined, by = "id")
combined <- merge(chart_df, audience, by = "id", all = TRUE)

write.csv(combined,"C:/Users/caitl/Documents/GitHub/thesis/exports/unsdglearn.csv", row.names = TRUE)


```

```{r}

authors_list <- as.data.frame(authors_list)

write.csv(authors_list,"C:/Users/caitl/Documents/GitHub/mpp_thesis/exports/authors_list.csv", row.names = TRUE)

au_category_clean

au_category_list <- as.data.frame(au_category_clean)

write.csv(au_category_list,"C:/Users/caitl/Documents/GitHub/mpp_thesis/exports/au_category_list.csv", row.names = TRUE)


write.csv(results_urls,"C:/Users/caitl/Documents/GitHub/mpp_thesis/exports/results_urls.csv", row.names = TRUE)

write.csv(au_category_clean,"C:/Users/caitl/Documents/GitHub/mpp_thesis/exports/au_category_clean.csv", row.names = TRUE)


write.csv(authors_list_A,"C:/Users/caitl/Documents/GitHub/mpp_thesis/exports/authors_list_A.csv", row.names = TRUE)

```

```
